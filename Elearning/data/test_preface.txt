Dynamic programming, like the divide-and-conquer method, solves problems by
combining the solutions to subproblems. ("Programming" in this context refers
to a tabular method, not to writing computer code.) As we saw in Chapters 2
and 4, divide-and-conquer algorithms partition the problem into disjoint subprob-
lems, solve the subproblems recursively, and then combine their solutions to solve
the original problem. In contrast, dynamic programming applies when the subprob-
lems overlap-that is, when subproblems share subsubproblems. In this context,
a divide-and-conquer algorithm does more work than necessary, repeatedly solv-
ing the common subsubproblems. A dynamic-programming algorithm solves each
subsubproblem just once and then saves its answer in a table, thereby avoiding the
work of recomputing the answer every time it solves each subsubproblem.
We typically apply dynamic programming tooptimizationproblems. Such prob-
lems can have many possible solutions. Each solution has a value, and we wish to
find a solution with the optimal (minimum or maximum) value. We call such a
solution an optimal solution to the problem, as opposed to the optimal solution,
since there may be several solutions that achieve the optimal value.
When developing a dynamic-programming algorithm, we follow a sequence of
four steps:
1. Characterize the structure of an optimal solution.
2. Recursively define the value of an optimal solution.
3. Compute the value of an optimal solution, typically in a bottom-up fashion.
4. Construct an optimal solution from computed information.
Steps 1–3 form the basis of a dynamic-programming solution to a problem. If we
need only the value of an optimal solution, and not the solution itself, then we
can omit step 4. When we do perform step 4, we sometimes maintain additional
information during step 3 so that we can easily construct an optimal solution.
The sections that follow use the dynamic-programming method to solve some
optimization problems. Section 15.1 examines the problem of cutting a rod into
360 Chapter 15 Dynamic Programming
rods of smaller length in way that maximizes their total value. Section 15.2 asks
how we can multiply a chain of matrices while performing the fewest total scalar
multiplications. Given these examples of dynamic programming, Section 15.3 dis-
cusses two key characteristics that a problem must have for dynamic programming
to be a viable solution technique. Section 15.4 then shows how to find the longest
common subsequence of two sequences via dynamic programming. Finally, Sec-
tion 15.5 uses dynamic programming to construct binary search trees that are opti-
mal, given a known distribution of keys to be looked up.
15.1 Rodcutting
Our first example uses dynamic programming to solve a simple problem in decid-
ing where to cut steel rods. Serling Enterprises buys long steel rods and cuts them
into shorter rods, which it then sells. Each cut is free. The management of Serling
Enterprises wants to know the best way to cut up the rods.
We assume that we know, for iD 1;2;:::, the price p
i
in dollars that Serling
Enterprises charges for a rod of length i inches. Rod lengths are always an integral
number of inches. Figure 15.1 gives a sample price table.
Therod-cuttingproblem is the following. Given a rod of length n inches and a
table of prices p
i
for iD 1;2;:::;n, determine the maximum revenue r
n
obtain-
able by cutting up the rod and selling the pieces. Note that if the price p
n
for a rod
of length n is large enough, an optimal solution may require no cutting at all.
Consider the case when nD 4. Figure 15.2 shows all the ways to cut up a rod
of 4 inches in length, including the way with no cuts at all. We see that cutting a
4-inch rod into two 2-inch pieces produces revenue p
2
C p
2
D 5C 5D 10,which
is optimal.
We can cut up a rod of length n in 2
n1
different ways, since we have an in-
dependent option of cutting, or not cutting, at distance i inches from the left end,
length i 1234 5 6 7 8 9 10
price p
i
15891017 1720 2430
Figure 15.1 A sample price table for rods. Each rod of length i inches earns the company p
i
dollars of revenue.
15.1 Rod cutting 361
9
(a)
1
(b)
8
(c) (d)
(e) (f) (g)
1
(h)
111
55 1 8
5 1151 151 1
Figure 15.2 The 8 possible ways of cutting up a rod of length 4. Above each piece is the
value of that piece, according to the sample price chart of Figure 15.1. The optimal strategy is
part (c)-cutting the rod into two pieces of length 2-which has total value 10.
for iD 1;2;:::;n 1.
1
We denote a decomposition into pieces using ordinary
additive notation, so that 7D 2C 2C 3 indicates that a rod of length 7 is cut into
three pieces-two of length 2 and one of length 3. If an optimal solution cuts the
rod into k pieces, for some 1 k n, then an optimal decomposition
nD i
1
C i
2
CC i
k
of the rod into pieces of lengths i
1
, i
2
, ..., i
k
provides maximum corresponding
revenue
r
n
D p
i
1
C p
i
2
CC p
i
k
:
For our sample problem, we can determine the optimal revenue figures r
i
,for
iD 1;2;:::;10, by inspection, with the corresponding optimal decompositions
1
If we required the pieces to be cut in order of nondecreasing size, there would be fewer ways
to consider. For n D 4, we would consider only 5 such ways: parts (a), (b), (c), (e), and (h)
in Figure 15.2. The number of ways is called the partition function; it is approximately equal to
e

p
2n=3
=4n
p
3. This quantity is less than 2
n1
, but still much greater than any polynomial in n.
We shall not pursue this line of inquiry further, however.
362 Chapter 15 Dynamic Programming
r
1
D 1 from solution 1D 1 (no cuts) ;
r
2
D 5 from solution 2D 2 (no cuts) ;
r
3
D 8 from solution 3D 3 (no cuts) ;
r
4
D 10 from solution 4D 2C2;
r
5
D 13 from solution 5D 2C3;
r
6
D 17 from solution 6D 6 (no cuts) ;
r
7
D 18 from solution 7D 1C 6 or 7D 2C 2C3;
r
8
D 22 from solution 8D 2C6;
r
9
D 25 from solution 9D 3C6;
r
10
D 30 from solution 10D 10 (no cuts) :
More generally, we can frame the values r
n
for n 1 in terms of optimal rev-
enues from shorter rods:
r
n
D max .p
n
;r
1
C r
n1
;r
2
C r
n2
;:::;r
n1
C r
1
/: (15.1)
The first argument, p
n
, corresponds to making no cuts at all and selling the rod of
length n as is. The other n 1 arguments to max correspond to the maximum rev-
enue obtained by making an initial cut of the rod into two pieces of size i and n i,
for each iD 1;2;:::;n 1, and then optimally cutting up those pieces further,
obtaining revenues r
i
and r
ni
from those two pieces. Since we don’t know ahead
of time which value of i optimizes revenue, we have to consider all possible values
for i and pick the one that maximizes revenue. We also have the option of picking
no i at all if we can obtain more revenue by selling the rod uncut.
Note that to solve the original problem of size n, we solve smaller problems of
the same type, but of smaller sizes. Once we make the first cut, we may consider
the two pieces as independent instances of the rod-cutting problem. The overall
optimal solution incorporates optimal solutions to the two related subproblems,
maximizing revenue from each of those two pieces. We say that the rod-cutting
problem exhibitsoptimalsubstructure: optimal solutions to a problem incorporate
optimal solutions to related subproblems, which we may solve independently.
In a related, but slightly simpler, way to arrange a recursive structure for the rod-
cutting problem, we view a decomposition as consisting of a first piece of length i
cut off the left-hand end, and then a right-hand remainder of length n i.Only
the remainder, and not the first piece, may be further divided. We may view every
decomposition of a length-n rod in this way: as a first piece followed by some
decomposition of the remainder. When doing so, we can couch the solution with
no cuts at all as saying that the first piece has size iD n and revenue p
n
and that
the remainder has size 0 with corresponding revenue r
0
D 0. We thus obtain the
following simpler version of equation (15.1):
r
n
D max
1in
.p
i
C r
ni
/: (15.2)
15.1 Rod cutting 363
In this formulation, an optimal solution embodies the solution to only one related
subproblem-the remainder-rather than two.
Recursivetop-downimplementation
The following procedure implements the computation implicit in equation (15.2)
in a straightforward, top-down, recursive manner.
CUT-ROD.p; n/
1 if n == 0
2 return 0
3 qD1
4 for iD 1to n
5 qD max.q; pŒiC CUT-ROD.p; n i//
6 return q
Procedure CUT-ROD takes as input an array pŒ1: :n of prices and an integer n,
and it returns the maximum revenue possible for a rod of length n.If nD 0,no
revenue is possible, and so CUT-ROD returns 0 in line 2. Line 3 initializes the
maximum revenue q to1, so that the for loop in lines 4–5 correctly computes
qD max
1in
.p
i
C CUT-ROD.p; n i//; line 6 then returns this value. A simple
induction on n proves that this answer is equal to the desired answer r
n
,using
equation (15.2).
If you were to code up CUT-ROD in your favorite programming language and run
it on your computer, you would find that once the input size becomes moderately
large, your program would take a long time to run. For nD 40, you would find that
your program takes at least several minutes, and most likely more than an hour. In
fact, you would find that each time you increase n by 1, your program’s running
time would approximately double.
Why is CUT-ROD so inefficient? The problem is that CUT-ROD calls itself
recursively over and over again with the same parameter values; it solves the
same subproblems repeatedly. Figure 15.3 illustrates what happens for nD 4:
CUT-ROD.p; n/ calls CUT-ROD.p; n i/ for i D 1;2;:::;n. Equivalently,
CUT-ROD.p; n/ calls CUT-ROD.p; j / for each j D 0; 1; : : : ; n 1. When this
process unfolds recursively, the amount of work done, as a function of n,grows
explosively.
To analyze the running time of CUT-ROD,let T .n/ denote the total number of
calls made to CUT-ROD when called with its second parameter equal to n.This
expression equals the number of nodes in a subtree whose root is labeled n in the
recursion tree. The count includes the initial call at its root. Thus, T.0/D 1 and
364 Chapter 15 Dynamic Programming
3
1 0
0
0
0 1
2 0
0
1
2
0
1 0
4
Figure 15.3 The recursion tree showing recursive calls resulting from a call CUT-ROD.p; n/ for
nD 4. Each node label gives the size n of the corresponding subproblem, so that an edge from
a parent with label s to a child with label t corresponds to cutting off an initial piece of size s t
and leaving a remaining subproblem of size t. A path from the root to a leaf corresponds to one of
the 2
n1
ways of cutting up a rod of length n. In general, this recursion tree has 2
n
nodes and 2
n1
leaves.
T .n/D 1C
n1
X
j D0
T.j/ : (15.3)
The initial 1 is for the call at the root, and the term T.j/ counts the number of calls
(including recursive calls) due to the call CUT-ROD.p; n i/,where jD n i.
As Exercise 15.1-1 asks you to show,
T .n/D 2
n
; (15.4)
and so the running time of CUT-ROD is exponential in n.
In retrospect, this exponential running time is not so surprising. CUT-ROD ex-
plicitly considers all the 2
n1
possible ways of cutting up a rod of length n.The
tree of recursive calls has 2
n1
leaves, one for each possible way of cutting up the
rod. The labels on the simple path from the root to a leaf give the sizes of each
remaining right-hand piece before making each cut. That is, the labels give the
corresponding cut points, measured from the right-hand end of the rod.
Usingdynamicprogrammingforoptimalrodcutting
We now show how to convert CUT-ROD into an efficient algorithm, using dynamic
programming.
The dynamic-programming method works as follows. Having observed that a
naive recursive solution is inefficient because it solves the same subproblems re-
peatedly, we arrange for each subproblem to be solved only once, saving its solu-
tion. If we need to refer to this subproblem’s solution again later, we can just look it
15.1 Rod cutting 365
up, rather than recompute it. Dynamic programming thus uses additional memory
to save computation time; it serves an example of atime-memorytrade-off.The
savings may be dramatic: an exponential-time solution may be transformed into a
polynomial-time solution. A dynamic-programming approach runs in polynomial
time when the number of distinct subproblems involved is polynomial in the input
size and we can solve each such subproblem in polynomial time.
There are usually two equivalent ways to implement a dynamic-programming
approach. We shall illustrate both of them with our rod-cutting example.
The first approach istop-down withmemoization.
2
In this approach, we write
the procedure recursively in a natural manner, but modified to save the result of
each subproblem (usually in an array or hash table). The procedure now first checks
to see whether it has previously solved this subproblem. If so, it returns the saved
value, saving further computation at this level; if not, the procedure computes the
value in the usual manner. We say that the recursive procedure has beenmemoized;
it "remembers" what results it has computed previously.
The second approach is thebottom-upmethod. This approach typically depends
on some natural notion of the "size" of a subproblem, such that solving any par-
ticular subproblem depends only on solving "smaller" subproblems. We sort the
subproblems by size and solve them in size order, smallest first. When solving a
particular subproblem, we have already solved all of the smaller subproblems its
solution depends upon, and we have saved their solutions. We solve each sub-
problem only once, and when we first see it, we have already solved all of its
prerequisite subproblems.
These two approaches yield algorithms with the same asymptotic running time,
except in unusual circumstances where the top-down approach does not actually
recurse to examine all possible subproblems. The bottom-up approach often has
much better constant factors, since it has less overhead for procedure calls.
Here is the the pseudocode for the top-down CUT-ROD procedure, with memo-
ization added:
MEMOIZED-CUT-ROD.p; n/
1let rŒ0::n be a new array
2 for iD 0to n
3 rŒiD1
4 return MEMOIZED-CUT-ROD-AUX.p;n;r/
2
This is not a misspelling. The word really is memoization, not memorization. Memoization comes
from memo, since the technique consists of recording a value so that we can look it up later.
366 Chapter 15 Dynamic Programming
MEMOIZED-CUT-ROD-AUX.p;n;r/
1 if rŒn 0
2 return rŒn
3 if n == 0
4 qD 0
5 else qD1
6 for iD 1to n
7 qD max.q; pŒiC MEMOIZED-CUT-ROD-AUX.p; n i;r//
8 rŒnD q
9 return q
Here, the main procedure MEMOIZED-CUT-ROD initializes a new auxiliary ar-
ray rŒ0::n with the value1, a convenient choice with which to denote "un-
known." (Known revenue values are always nonnegative.) It then calls its helper
routine, MEMOIZED-CUT-ROD-AUX.
The procedure MEMOIZED-CUT-ROD-AUX is just the memoized version of our
previous procedure, CUT-ROD. It first checks in line 1 to see whether the desired
value is already known and, if it is, then line 2 returns it. Otherwise, lines 3–7
compute the desired value q in the usual manner, line 8 saves it in rŒn, and line 9
returns it.
The bottom-up version is even simpler:
BOTTOM-UP-CUT-ROD.p; n/
1let rŒ0::n beanew array
2 rŒ0D 0
3 for j D 1to n
4 qD1
5 for iD 1to j
6 qD max.q; pŒiC rŒj i/
7 rŒjD q
8 return rŒn
For the bottom-up dynamic-programming approach, BOTTOM-UP-CUT-ROD
uses the natural ordering of the subproblems: a problem of size i is "smaller"
than a subproblem of size j ifi<j . Thus, the procedure solves subproblems of
sizes jD 0; 1; : : : ; n,inthatorder.
Line 1 of procedure BOTTOM-UP-CUT-ROD creates a new array rŒ0::n in
which to save the results of the subproblems, and line 2 initializes rŒ0 to 0,since
a rod of length 0 earns no revenue. Lines 3–6 solve each subproblem of size j,for
jD 1;2;:::;n, in order of increasing size. The approach used to solve a problem
of a particular size j is the same as that used by CUT-ROD, except that line 6 now
15.1 Rod cutting 367
3
0
1
2
4
Figure 15.4 The subproblem graph for the rod-cutting problem with nD 4. The vertex labels
give the sizes of the corresponding subproblems. A directed edge .x; y/ indicates that we need a
solution to subproblem y when solving subproblem x. This graph is a reduced version of the tree of
Figure 15.3, in which all nodes with the same label are collapsed into a single vertex and all edges
go from parent to child.
directly references array entry rŒj i instead of making a recursive call to solve
the subproblem of size j i. Line 7 saves in rŒj the solution to the subproblem
of size j . Finally, line 8 returns rŒn, which equals the optimal value r
n
.
The bottom-up and top-down versions have the same asymptotic running time.
The running time of procedure BOTTOM-UP-CUT-ROD is ‚.n
2
/, due to its
doubly-nested loop structure. The number of iterations of its inner for loop, in
lines 5–6, forms an arithmetic series. The running time of its top-down counterpart,
MEMOIZED-CUT-ROD,isalso ‚.n
2
/, although this running time may be a little
harder to see. Because a recursive call to solve a previously solved subproblem
returns immediately, MEMOIZED-CUT-ROD solves each subproblem just once. It
solves subproblems for sizes 0; 1; : : : ; n. To solve a subproblem of size n,thefor
loop of lines 6–7 iterates n times. Thus, the total number of iterations of this for
loop, over all recursive calls of MEMOIZED-CUT-ROD, forms an arithmetic series,
giving a total of ‚.n
2
/ iterations, just like the inner for loop of BOTTOM-UP-
CUT-ROD. (We actually are using a form of aggregate analysis here. We shall see
aggregate analysis in detail in Section 17.1.)
Subproblemgraphs
When we think about a dynamic-programming problem, we should understand the
set of subproblems involved and how subproblems depend on one another.
Thesubproblemgraph for the problem embodies exactly this information. Fig-
ure 15.4 shows the subproblem graph for the rod-cutting problem with nD 4.It
is a directed graph, containing one vertex for each distinct subproblem. The sub-
368 Chapter 15 Dynamic Programming
problem graph has a directed edge from the vertex for subproblem x to the vertex
for subproblem y if determining an optimal solution for subproblem x involves
directly considering an optimal solution for subproblem y. For example, the sub-
problem graph contains an edge from x to y if a top-down recursive procedure for
solving x directly calls itself to solve y. We can think of the subproblem graph
as a "reduced" or "collapsed" version of the recursion tree for the top-down recur-
sive method, in which we coalesce all nodes for the same subproblem into a single
vertex and direct all edges from parent to child.
The bottom-up method for dynamic programming considers the vertices of the
subproblem graph in such an order that we solve the subproblems y adjacent to
a given subproblem x before we solve subproblem x. (Recall from Section B.4
that the adjacency relation is not necessarily symmetric.) Using the terminology
from Chapter 22, in a bottom-up dynamic-programming algorithm, we consider the
vertices of the subproblem graph in an order that is a "reverse topological sort," or
a "topological sort of the transpose" (see Section 22.4) of the subproblem graph. In
other words, no subproblem is considered until all of the subproblems it depends
upon have been solved. Similarly, using notions from the same chapter, we can
view the top-down method (with memoization) for dynamic programming as a
"depth-first search" of the subproblem graph (see Section 22.3).
The size of the subproblem graph GD .V; E/ can help us determine the running
time of the dynamic programming algorithm. Since we solve each subproblem just
once, the running time is the sum of the times needed to solve each subproblem.
Typically, the time to compute the solution to a subproblem is proportional to the
degree (number of outgoing edges) of the corresponding vertex in the subproblem
graph, and the number of subproblems is equal to the number of vertices in the sub-
problem graph. In this common case, the running time of dynamic programming
is linear in the number of vertices and edges.
Reconstructingasolution
Our dynamic-programming solutions to the rod-cutting problem return the value of
an optimal solution, but they do not return an actual solution: a list of piece sizes.
We can extend the dynamic-programming approach to record not only the optimal
value computed for each subproblem, but also a choice that led to the optimal
value. With this information, we can readily print an optimal solution.
Here is an extended version of BOTTOM-UP-CUT-ROD that computes, for each
rod size j , not only the maximum revenue r
j
, but also s
j
, the optimal size of the
first piece to cut off:
15.1 Rod cutting 369
EXTENDED-BOTTOM-UP-CUT-ROD.p; n/
1let rŒ0::n and sŒ0 : : n be new arrays
2 rŒ0D 0
3 for jD 1to n
4 qD1
5 for iD 1to j
6 ifq<pŒiC rŒj i
7 qD pŒiC rŒj i
8 sŒjD i
9 rŒjD q
10 return r and s
This procedure is similar to BOTTOM-UP-CUT-ROD, except that it creates the ar-
ray s in line 1, and it updates sŒj in line 8 to hold the optimal size i of the first
piece to cut off when solving a subproblem of size j .
The following procedure takes a price table p and a rod size n, and it calls
EXTENDED-BOTTOM-UP-CUT-ROD to compute the array sŒ1 : : n of optimal
first-piece sizes and then prints out the complete list of piece sizes in an optimal
decomposition of a rod of length n:
PRINT-CUT-ROD-SOLUTION.p; n/
1 .r; s/D EXTENDED-BOTTOM-UP-CUT-ROD.p; n/
2 whilen>0
3 print sŒn
4 nD n sŒn
In our rod-cutting example, the call EXTENDED-BOTTOM-UP-CUT-ROD.p; 10/
would return the following arrays:
i 0 1 2 3 456789 10
rŒi 0 1 5 8 10 13 17 18 22 25 30
sŒi 0 1 2 3 226123 10
A call to PRINT-CUT-ROD-SOLUTION.p; 10/ would print just 10, but a call with
nD 7 would print the cuts 1 and 6, corresponding to the first optimal decomposi-
tion for r
7
given earlier.
Exercises
15.1-1
Show that equation (15.4) follows from equation (15.3) and the initial condition
T.0/D 1.
370 Chapter 15 Dynamic Programming
15.1-2
Show, by means of a counterexample, that the following "greedy" strategy does
not always determine an optimal way to cut rods. Define thedensity of a rod of
length i to be p
i
=i, that is, its value per inch. The greedy strategy for a rod of
length n cuts off a first piece of length i,where 1 i  n, having maximum
density. It then continues by applying the greedy strategy to the remaining piece of
length n i.
15.1-3
Consider a modification of the rod-cutting problem in which, in addition to a
price p
i
for each rod, each cut incurs a fixed cost of c. The revenue associated with
a solution is now the sum of the prices of the pieces minus the costs of making the
cuts. Give a dynamic-programming algorithm to solve this modified problem.
15.1-4
Modify MEMOIZED-CUT-ROD to return not only the value but the actual solution,
too.
15.1-5
The Fibonacci numbers are defined by recurrence (3.22). Give an O.n/-time
dynamic-programming algorithm to compute the nth Fibonacci number. Draw the
subproblem graph. How many vertices and edges are in the graph?
15.2 Matrix-chainmultiplication
Our next example of dynamic programming is an algorithm that solves the problem
of matrix-chain multiplication. We are given a sequence (chain)hA
1
;A
2
;:::;A
n
i
of n matrices to be multiplied, and we wish to compute the product
A
1
A
2
 A
n
: (15.5)
We can evaluate the expression (15.5) using the standard algorithm for multiply-
ing pairs of matrices as a subroutine once we have parenthesized it to resolve all
ambiguities in how the matrices are multiplied together. Matrix multiplication is
associative, and so all parenthesizations yield the same product. A product of ma-
trices isfullyparenthesized if it is either a single matrix or the product of two fully
parenthesized matrix products, surrounded by parentheses. For example, if the
chain of matrices ishA
1
;A
2
;A
3
;A
4
i, then we can fully parenthesize the product
A
1
A
2
A
3
A
4
in five distinct ways:
15.2 Matrix-chain multiplication 371
.A
1
.A
2
.A
3
A
4
/// ;
.A
1
..A
2
A
3
/A
4
// ;
..A
1
A
2
/.A
3
A
4
// ;
..A
1
.A
2
A
3
//A
4
/;
...A
1
A
2
/A
3
/A
4
/:
How we parenthesize a chain of matrices can have a dramatic impact on the cost
of evaluating the product. Consider first the cost of multiplying two matrices. The
standard algorithm is given by the following pseudocode, which generalizes the
SQUARE-MATRIX-MULTIPLY procedure from Section 4.2. The attributes rows
and columns are the numbers of rows and columns in a matrix.
MATRIX-MULTIPLY.A; B/
1 if A:columns¤ B:rows
2 error "incompatible dimensions"
3 else let C beanew A:rows	 B:columns matrix
4 for iD 1to A:rows
5 for jD 1to B:columns
6 c
ij
D 0
7 for kD 1to A:columns
8 c
ij
D c
ij
C a
ik
 b
kj
9 return C
We can multiply two matrices A and B only if they arecompatible: the number of
columns of A must equal the number of rows of B.If A is a p	 q matrix and B is
a q	 r matrix, the resulting matrix C is a p	 r matrix. The time to compute C is
dominated by the number of scalar multiplications in line 8, which is pqr.Inwhat
follows, we shall express costs in terms of the number of scalar multiplications.
To illustrate the different costs incurred by different parenthesizations of a matrix
product, consider the problem of a chainhA
1
;A
2
;A
3
i of three matrices. Suppose
that the dimensions of the matrices are 10	 100, 100	 5,and 5	 50, respec-
tively. If we multiply according to the parenthesization ..A
1
A
2
/A
3
/, we perform
10 100 5D 5000 scalar multiplications to compute the 10	 5 matrix prod-
uct A
1
A
2
, plus another 10 5 50D 2500 scalar multiplications to multiply this
matrix by A
3
, for a total of 7500 scalar multiplications. If instead we multiply
according to the parenthesization .A
1
.A
2
A
3
//, we perform 100 5 50D 25,000
scalar multiplications to compute the 100	 50 matrix product A
2
A
3
, plus another
10 100 50D 50,000 scalar multiplications to multiply A
1
by this matrix, for a
total of 75,000 scalar multiplications. Thus, computing the product according to
the first parenthesization is 10 times faster.
We state the matrix-chain multiplication problem as follows: given a chain
hA
1
;A
2
;:::;A
n
i of n matrices, where for iD 1;2;:::;n, matrix A
i
has dimension
372 Chapter 15 Dynamic Programming
p
i1
	 p
i
, fully parenthesize the product A
1
A
2
 A
n
in a way that minimizes the
number of scalar multiplications.
Note that in the matrix-chain multiplication problem, we are not actually multi-
plying matrices. Our goal is only to determine an order for multiplying matrices
that has the lowest cost. Typically, the time invested in determining this optimal
order is more than paid for by the time saved later on when actually performing the
matrix multiplications (such as performing only 7500 scalar multiplications instead
of 75,000).
Countingthenumberofparenthesizations
Before solving the matrix-chain multiplication problem by dynamic programming,
let us convince ourselves that exhaustively checking all possible parenthesizations
does not yield an efficient algorithm. Denote the number of alternative parenthe-
sizations of a sequence of n matrices by P.n/.When nD 1, we have just one
matrix and therefore only one way to fully parenthesize the matrix product. When
n 2, a fully parenthesized matrix product is the product of two fully parenthe-
sized matrix subproducts, and the split between the two subproducts may occur
between the kth and .kC 1/st matrices for any kD 1;2;:::;n 1. Thus, we
obtain the recurrence
P.n/D

1 if nD 1;
n1
X
kD1
P.k/P.n k/ if n 2:
(15.6)
Problem 12-4 asked you to show that the solution to a similar recurrence is the
sequence of Catalan numbers, which grows as .4
n
=n
3=2
/. A simpler exercise
(see Exercise 15.2-3) is to show that the solution to the recurrence (15.6) is .2
n
/.
The number of solutions is thus exponential in n, and the brute-force method of
exhaustive search makes for a poor strategy when determining how to optimally
parenthesize a matrix chain.
Applyingdynamicprogramming
We shall use the dynamic-programming method to determine how to optimally
parenthesize a matrix chain. In so doing, we shall follow the four-step sequence
that we stated at the beginning of this chapter:
1. Characterize the structure of an optimal solution.
2. Recursively define the value of an optimal solution.
3. Compute the value of an optimal solution.
15.2 Matrix-chain multiplication 373
4. Construct an optimal solution from computed information.
We shall go through these steps in order, demonstrating clearly how we apply each
step to the problem.
Step1: Thestructureofanoptimalparenthesization
For our first step in the dynamic-programming paradigm, we find the optimal sub-
structure and then use it to construct an optimal solution to the problem from opti-
mal solutions to subproblems. In the matrix-chain multiplication problem, we can
perform this step as follows. For convenience, let us adopt the notation A
i::j
,where
i j , for the matrix that results from evaluating the product A
i
A
iC1
 A
j
.Ob-
serve that if the problem is nontrivial, i.e.,i<j , then to parenthesize the product
A
i
A
iC1
 A
j
, we must split the product between A
k
and A
kC1
for some integer k
in the range ik<j . That is, for some value of k, we first compute the matrices
A
i::k
and A
kC1::j
and then multiply them together to produce the final product A
i::j
.
The cost of parenthesizing this way is the cost of computing the matrix A
i::k
,plus
the cost of computing A
kC1::j
, plus the cost of multiplying them together.
The optimal substructure of this problem is as follows. Suppose that to op-
timally parenthesize A
i
A
iC1
 A
j
, we split the product between A
k
and A
kC1
.
Then the way we parenthesize the "prefix" subchain A
i
A
iC1
 A
k
within this
optimal parenthesization of A
i
A
iC1
 A
j
must be an optimal parenthesization of
A
i
A
iC1
 A
k
. Why? If there were a less costly way to parenthesize A
i
A
iC1
 A
k
,
then we could substitute that parenthesization in the optimal parenthesization
of A
i
A
iC1
 A
j
to produce another way to parenthesize A
i
A
iC1
 A
j
whose cost
was lower than the optimum: a contradiction. A similar observation holds for how
we parenthesize the subchain A
kC1
A
kC2
 A
j
in the optimal parenthesization of
A
i
A
iC1
 A
j
: it must be an optimal parenthesization of A
kC1
A
kC2
 A
j
.
Now we use our optimal substructure to show that we can construct an optimal
solution to the problem from optimal solutions to subproblems. We have seen that
any solution to a nontrivial instance of the matrix-chain multiplication problem
requires us to split the product, and that any optimal solution contains within it op-
timal solutions to subproblem instances. Thus, we can build an optimal solution to
an instance of the matrix-chain multiplication problem by splitting the problem into
two subproblems (optimally parenthesizing A
i
A
iC1
 A
k
and A
kC1
A
kC2
 A
j
),
finding optimal solutions to subproblem instances, and then combining these op-
timal subproblem solutions. We must ensure that when we search for the correct
place to split the product, we have considered all possible places, so that we are
sure of having examined the optimal one.
374 Chapter 15 Dynamic Programming
Step2: Arecursivesolution
Next, we define the cost of an optimal solution recursively in terms of the optimal
solutions to subproblems. For the matrix-chain multiplication problem, we pick as
our subproblems the problems of determining the minimum cost of parenthesizing
A
i
A
iC1
 A
j
for 1 i j n.Let mŒi; j  be the minimum number of scalar
multiplications needed to compute the matrix A
i::j
; for the full problem, the lowest-
cost way to compute A
1::n
would thus be mŒ1; n.
We can define mŒi; j  recursively as follows. If iD j , the problem is trivial;
the chain consists of just one matrix A
i::i
D A
i
, so that no scalar multiplications
are necessary to compute the product. Thus, mŒi; iD 0 for iD 1;2;:::;n.To
compute mŒi; j  wheni< j , we take advantage of the structure of an optimal
solution from step 1. Let us assume that to optimally parenthesize, we split the
product A
i
A
iC1
 A
j
between A
k
and A
kC1
,where i  k< j . Then, mŒi; j 
equals the minimum cost for computing the subproducts A
i::k
and A
kC1::j
,plusthe
cost of multiplying these two matrices together. Recalling that each matrix A
i
is
p
i1
	 p
i
, we see that computing the matrix product A
i::k
A
kC1::j
takes p
i1
p
k
p
j
scalar multiplications. Thus, we obtain
mŒi; j D mŒi; kC mŒkC 1; j C p
i1
p
k
p
j
:
This recursive equation assumes that we know the value of k, which we do not.
There are only ji possible values for k, however, namely kD i;iC1;:::;j1.
Since the optimal parenthesization must use one of these values for k, we need only
check them all to find the best. Thus, our recursive definition for the minimum cost
of parenthesizing the product A
i
A
iC1
 A
j
becomes
mŒi; j D
(
0 if iDj;
min
ik<j
fmŒi; kC mŒkC 1; j C p
i1
p
k
p
j
g ifi<j :
(15.7)
The mŒi; j  values give the costs of optimal solutions to subproblems, but they
do not provide all the information we need to construct an optimal solution. To
help us do so, we define sŒi;j to be a value of k at which we split the product
A
i
A
iC1
 A
j
in an optimal parenthesization. That is, sŒi;j equals a value k such
that mŒi; j D mŒi; kC mŒkC 1; j C p
i1
p
k
p
j
.
Step3: Computingtheoptimalcosts
At this point, we could easily write a recursive algorithm based on recurrence (15.7)
to compute the minimum cost mŒ1; n for multiplying A
1
A
2
 A
n
.Aswesawfor
the rod-cutting problem, and as we shall see in Section 15.3, this recursive algo-
rithm takes exponential time, which is no better than the brute-force method of
checking each way of parenthesizing the product.
15.2 Matrix-chain multiplication 375
Observe that we have relatively few distinct subproblems: one subproblem for
each choice of i and j satisfying 1 i j n,or

n
2

C n D ‚.n
2
/ in all.
A recursive algorithm may encounter each subproblem many times in different
branches of its recursion tree. This property of overlapping subproblems is the
second hallmark of when dynamic programming applies (the first hallmark being
optimal substructure).
Instead of computing the solution to recurrence (15.7) recursively, we compute
the optimal cost by using a tabular, bottom-up approach. (We present the corre-
sponding top-down approach using memoization in Section 15.3.)
We shall implement the tabular, bottom-up method in the procedure MATRIX-
CHAIN-ORDER, which appears below. This procedure assumes that matrix A
i
has dimensions p
i1
	 p
i
for i D 1;2;:::;n. Its input is a sequence p D
hp
0
;p
1
;:::;p
n
i,where p:length D nC 1. The procedure uses an auxiliary
table mŒ1::n;1::n for storing the mŒi; j  costs and another auxiliary table
sŒ1::n 1;2::n that records which index of k achieved the optimal cost in com-
puting mŒi; j . We shall use the table s to construct an optimal solution.
In order to implement the bottom-up approach, we must determine which entries
of the table we refer to when computing mŒi; j . Equation (15.7) shows that the
cost mŒi; j  of computing a matrix-chain product of jiC1 matrices depends only
on the costs of computing matrix-chain products of fewer than j iC 1 matrices.
That is, for kD i;iC 1;:::;j 1, the matrix A
i::k
is a product of k iC1<
j iC 1 matrices and the matrix A
kC1::j
is a product of jk< j iC 1
matrices. Thus, the algorithm should fill in the table m in a manner that corresponds
to solving the parenthesization problem on matrix chains of increasing length. For
the subproblem of optimally parenthesizing the chain A
i
A
iC1
 A
j
, we consider
the subproblem size to be the length j iC 1 of the chain.
MATRIX-CHAIN-ORDER.p/
1 nD p:length 1
2let mŒ1::n;1::n and sŒ1::n 1;2::n be new tables
3 for iD 1to n
4 mŒi; iD 0
5 for lD 2to n // l is the chain length
6 for iD 1to n lC 1
7 jD iC l 1
8 mŒi; j D1
9 for kD i to j 1
10 qD mŒi; kC mŒkC 1; j C p
i1
p
k
p
j
11 ifq<mŒi;j
12 mŒi; j D q
13 sŒi;jD k
14 return m and s
376 Chapter 15 Dynamic Programming
A
6
A
5
A
4
A
3
A
2
A
1
00 00 00
15,750 2,625 750 1,000 5,000
7,875 4,375 2,500 3,500
9,375 7,125 5,375
11,875 10,500
15,125
1
2
3
4
5
61
2
3
4
5
6
ji
m
12 34 5
13 35
33 3
33
3
2
3
4
5
61
2
3
4
5
ji
s
Figure15.5 The m and s tables computed by MATRIX-CHAIN-ORDER for nD 6 and the follow-
ingmatrixdimensions:
matrix A
1
A
2
A
3
A
4
A
5
A
6
dimension 30	 35 35	 15 15	55	 10 10	 20 20	 25
The tables are rotated so that the main diagonal runs horizontally. The m table uses only the main
diagonal and upper triangle, and the s table uses only the upper triangle. The minimum number of
scalar multiplications to multiply the 6 matrices is mŒ1; 6D 15,125. Of the darker entries, the pairs
that have the same shading are taken together in line 10 when computing
mŒ2; 5 D min
8
ˆ
<
ˆ
:
mŒ2; 2C mŒ3; 5C p
1
p
2
p
5
D 0C 2500C 35 15 20 D 13,000 ;
mŒ2; 3C mŒ4; 5C p
1
p
3
p
5
D 2625C 1000C 35 5 20 D 7125 ;
mŒ2; 4C mŒ5; 5C p
1
p
4
p
5
D 4375C 0C 35 10 20 D 11,375
D 7125 :
The algorithm first computes mŒi; i D 0 for i D 1;2;:::;n (the minimum
costs for chains of length 1) in lines 3–4. It then uses recurrence (15.7) to compute
mŒi; iC 1 for iD 1;2;:::;n 1 (the minimum costs for chains of length lD 2)
during the first execution of thefor loop in lines 5–13. The second time through the
loop, it computes mŒi; iC2 for iD 1;2;:::;n2 (the minimum costs for chains of
length lD 3), and so forth. At each step, the mŒi; j  cost computed in lines 10–13
depends only on table entries mŒi; k and mŒkC 1; j  already computed.
Figure 15.5 illustrates this procedure on a chain of n D 6 matrices. Since
we have defined mŒi; j  only for i  j , only the portion of the table m strictly
above the main diagonal is used. The figure shows the table rotated to make the
main diagonal run horizontally. The matrix chain is listed along the bottom. Us-
ing this layout, we can find the minimum cost mŒi; j  for multiplying a subchain
A
i
A
iC1
 A
j
of matrices at the intersection of lines running northeast from A
i
and
15.2 Matrix-chain multiplication 377
northwest from A
j
. Each horizontal row in the table contains the entries for matrix
chains of the same length. MATRIX-CHAIN-ORDER computes the rows from bot-
tom to top and from left to right within each row. It computes each entry mŒi; j 
using the products p
i1
p
k
p
j
for kD i;iC1;:::;j 1 and all entries southwest
and southeast from mŒi; j .
A simple inspection of the nested loop structure of MATRIX-CHAIN-ORDER
yields a running time of O.n
3
/ for the algorithm. The loops are nested three deep,
and each loop index (l, i,and k) takes on at most n1 values. Exercise 15.2-5 asks
you to show that the running time of this algorithm is in fact also .n
3
/.Theal-
gorithm requires ‚.n
2
/ space to store the m and s tables. Thus, MATRIX-CHAIN-
ORDER is much more efficient than the exponential-time method of enumerating
all possible parenthesizations and checking each one.
Step4: Constructinganoptimalsolution
Although MATRIX-CHAIN-ORDER determines the optimal number of scalar mul-
tiplications needed to compute a matrix-chain product, it does not directly show
how to multiply the matrices. The table sŒ1 : : n 1;2::n gives us the informa-
tion we need to do so. Each entry sŒi;j records a value of k such that an op-
timal parenthesization of A
i
A
iC1
 A
j
splits the product between A
k
and A
kC1
.
Thus, we know that the final matrix multiplication in computing A
1::n
optimally
is A
1::sŒ1;n
A
sŒ1;nC1::n
. We can determine the earlier matrix multiplications recur-
sively, since sŒ1; sŒ1; n determines the last matrix multiplication when computing
A
1::sŒ1;n
and sŒsŒ1; nC 1; n determines the last matrix multiplication when com-
puting A
sŒ1;nC1::n
. The following recursive procedure prints an optimal parenthe-
sization ofhA
i
;A
iC1
; :::; A
j
i,given the s table computed by MATRIX-CHAIN-
ORDER and the indices i and j . The initial call PRINT-OPTIMAL-PARENS.s;1;n/
prints an optimal parenthesization ofhA
1
;A
2
;:::;A
n
i.
PRINT-OPTIMAL-PARENS.s;i;j/
1 if i == j
2 print "A"
i
3 else print "("
4PRINT-OPTIMAL-PARENS.s;i;sŒi;j/
5PRINT-OPTIMAL-PARENS.s;sŒi;jC 1; j /
6 print ")"
In the example of Figure 15.5, the call PRINT-OPTIMAL-PARENS.s;1;6/ prints
the parenthesization ..A
1
.A
2
A
3
//..A
4
A
5
/A
6
//.
378 Chapter 15 Dynamic Programming
Exercises
15.2-1
Find an optimal parenthesization of a matrix-chain product whose sequence of
dimensions ish5; 10; 3; 12; 5; 50; 6i.
15.2-2
Give a recursive algorithm MATRIX-CHAIN-MULTIPLY.A;s;i;j/ that actually
performs the optimal matrix-chain multiplication, given the sequence of matrices
hA
1
;A
2
;:::;A
n
i,the s table computed by MATRIX-CHAIN-ORDER, and the in-
dices i and j . (The initial call would be MATRIX-CHAIN-MULTIPLY.A;s;1;n/.)
15.2-3
Use the substitution method to show that the solution to the recurrence (15.6)
is .2
n
/.
15.2-4
Describe the subproblem graph for matrix-chain multiplication with an input chain
of length n. How many vertices does it have? How many edges does it have, and
which edges are they?
15.2-5
Let R.i; j / be the number of times that table entry mŒi; j  is referenced while
computing other table entries in a call of MATRIX-CHAIN-ORDER. Show that the
total number of references for the entire table is
n
X
iD1
n
X
j Di
R.i; j /D
n
3
 n
3
:
(Hint: You may find equation (A.3) useful.)
15.2-6
Show that a full parenthesization of an n-element expression has exactly n1 pairs
of parentheses.
15.3 Elementsofdynamicprogramming
Although we have just worked through two examples of the dynamic-programming
method, you might still be wondering just when the method applies. From an en-
gineering perspective, when should we look for a dynamic-programming solution
to a problem? In this section, we examine the two key ingredients that an opti-
15.3 Elements of dynamic programming 379
mization problem must have in order for dynamic programming to apply: optimal
substructure and overlapping subproblems. We also revisit and discuss more fully
how memoization might help us take advantage of the overlapping-subproblems
property in a top-down recursive approach.
Optimalsubstructure
The first step in solving an optimization problem by dynamic programming is to
characterize the structure of an optimal solution. Recall that a problem exhibits
optimalsubstructure if an optimal solution to the problem contains within it opti-
mal solutions to subproblems. Whenever a problem exhibits optimal substructure,
we have a good clue that dynamic programming might apply. (As Chapter 16 dis-
cusses, it also might mean that a greedy strategy applies, however.) In dynamic
programming, we build an optimal solution to the problem from optimal solutions
to subproblems. Consequently, we must take care to ensure that the range of sub-
problems we consider includes those used in an optimal solution.
We discovered optimal substructure in both of the problems we have examined
in this chapter so far. In Section 15.1, we observed that the optimal way of cut-
ting up a rod of length n (if we make any cuts at all) involves optimally cutting
up the two pieces resulting from the first cut. In Section 15.2, we observed that
an optimal parenthesization of A
i
A
iC1
 A
j
that splits the product between A
k
and A
kC1
contains within it optimal solutions to the problems of parenthesizing
A
i
A
iC1
 A
k
and A
kC1
A
kC2
 A
j
.
You will find yourself following a common pattern in discovering optimal sub-
structure:
1. You show that a solution to the problem consists of making a choice, such as
choosing an initial cut in a rod or choosing an index at which to split the matrix
chain. Making this choice leaves one or more subproblems to be solved.
2. You suppose that for a given problem, you are given the choice that leads to an
optimal solution. You do not concern yourself yet with how to determine this
choice. You just assume that it has been given to you.
3. Given this choice, you determine which subproblems ensue and how to best
characterize the resulting space of subproblems.
4. You show that the solutions to the subproblems used within an optimal solution
to the problem must themselves be optimal by using a "cut-and-paste" tech-
nique. You do so by supposing that each of the subproblem solutions is not
optimal and then deriving a contradiction. In particular, by "cutting out" the
nonoptimal solution to each subproblem and "pasting in" the optimal one, you
show that you can get a better solution to the original problem, thus contradict-
ing your supposition that you already had an optimal solution. If an optimal
380 Chapter 15 Dynamic Programming
solution gives rise to more than one subproblem, they are typically so similar
that you can modify the cut-and-paste argument for one to apply to the others
with little effort.
To characterize the space of subproblems, a good rule of thumb says to try to
keep the space as simple as possible and then expand it as necessary. For example,
the space of subproblems that we considered for the rod-cutting problem contained
the problems of optimally cutting up a rod of length i for each size i. This sub-
problem space worked well, and we had no need to try a more general space of
subproblems.
Conversely, suppose that we had tried to constrain our subproblem space for
matrix-chain multiplication to matrix products of the form A
1
A
2
 A
j
. As before,
an optimal parenthesization must split this product between A
k
and A
kC1
for some
1k<j . Unless we could guarantee that k always equals j 1, we would find
that we had subproblems of the form A
1
A
2
 A
k
and A
kC1
A
kC2
 A
j
,and that
the latter subproblem is not of the form A
1
A
2
 A
j
. For this problem, we needed
to allow our subproblems to vary at "both ends," that is, to allow both i and j to
vary in the subproblem A
i
A
iC1
 A
j
.
Optimal substructure varies across problem domains in two ways:
1. how many subproblems an optimal solution to the original problem uses, and
2. how many choices we have in determining which subproblem(s) to use in an
optimal solution.
In the rod-cutting problem, an optimal solution for cutting up a rod of size n
uses just one subproblem (of size n i), but we must consider n choices for i
in order to determine which one yields an optimal solution. Matrix-chain mul-
tiplication for the subchain A
i
A
iC1
 A
j
serves as an example with two sub-
problems and j i choices. For a given matrix A
k
at which we split the prod-
uct, we have two subproblems-parenthesizing A
i
A
iC1
 A
k
and parenthesizing
A
kC1
A
kC2
 A
j
-and we must solve both of them optimally. Once we determine
the optimal solutions to subproblems, we choose from among j i candidates for
the index k.
Informally, the running time of a dynamic-programming algorithm depends on
the product of two factors: the number of subproblems overall and how many
choices we look at for each subproblem. In rod cutting, we had ‚.n/ subproblems
overall, and at most n choices to examine for each, yielding an O.n
2
/ running time.
Matrix-chain multiplication had ‚.n
2
/ subproblems overall, and in each we had at
most n 1 choices, giving an O.n
3
/ running time (actually, a ‚.n
3
/ running time,
by Exercise 15.2-5).
Usually, the subproblem graph gives an alternative way to perform the same
analysis. Each vertex corresponds to a subproblem, and the choices for a sub-
15.3 Elements of dynamic programming 381
problem are the edges incident to that subproblem. Recall that in rod cutting,
the subproblem graph had n vertices and at most n edges per vertex, yielding an
O.n
2
/ running time. For matrix-chain multiplication, if we were to draw the sub-
problem graph, it would have ‚.n
2
/ vertices and each vertex would have degree at
most n 1, giving a total of O.n
3
/ vertices and edges.
Dynamic programming often uses optimal substructure in a bottom-up fashion.
That is, we first find optimal solutions to subproblems and, having solved the sub-
problems, we find an optimal solution to the problem. Finding an optimal solu-
tion to the problem entails making a choice among subproblems as to which we
will use in solving the problem. The cost of the problem solution is usually the
subproblem costs plus a cost that is directly attributable to the choice itself. In
rod cutting, for example, first we solved the subproblems of determining optimal
ways to cut up rods of length i for iD 0; 1; : : : ; n 1, and then we determined
which such subproblem yielded an optimal solution for a rod of length n,using
equation (15.2). The cost attributable to the choice itself is the term p
i
in equa-
tion (15.2). In matrix-chain multiplication, we determined optimal parenthesiza-
tions of subchains of A
i
A
iC1
 A
j
, and then we chose the matrix A
k
at which to
split the product. The cost attributable to the choice itself is the term p
i1
p
k
p
j
.
In Chapter 16, we shall examine "greedy algorithms," which have many similar-
ities to dynamic programming. In particular, problems to which greedy algorithms
apply have optimal substructure. One major difference between greedy algorithms
and dynamic programming is that instead of first finding optimal solutions to sub-
problems and then making an informed choice, greedy algorithms first make a
"greedy" choice-the choice that looks best at the time-and then solve a resulting
subproblem, without bothering to solve all possible related smaller subproblems.
Surprisingly, in some cases this strategy works!
Subtleties
You should be careful not to assume that optimal substructure applies when it does
not. Consider the following two problems in which we are given a directed graph
GD .V; E/ and vertices u; 2 V .
Unweightedshortestpath:
3
Find a path from u to  consisting of the fewest
edges. Such a path must be simple, since removing a cycle from a path pro-
duces a path with fewer edges.
3
We use the term "unweighted" to distinguish this problem from that of finding shortest paths with
weighted edges, which we shall see in Chapters 24 and 25. We can use the breadth-first search
technique of Chapter 22 to solve the unweighted problem.
382 Chapter 15 Dynamic Programming
q r
s t
Figure 15.6 A directed graph showing that the problem of finding a longest simple path in an
unweighted directed graph does not have optimal substructure. The path q! r! t is a longest
simple path from q to t, but the subpath q! r is not a longest simple path from q to r, nor is the
subpath r! t a longest simple path from r to t.
Unweightedlongestsimplepath: Find a simple path from u to  consisting of
the most edges. We need to include the requirement of simplicity because other-
wise we can traverse a cycle as many times as we like to create paths with an
arbitrarily large number of edges.
The unweighted shortest-path problem exhibits optimal substructure, as follows.
Suppose that u¤ , so that the problem is nontrivial. Then, any path p from u
to  must contain an intermediate vertex, say w. (Note that w may be u or .)
Thus, we can decompose the path u
p
  into subpaths u
p
1
 w
p
2
 . Clearly, the
number of edges in p equals the number of edges in p
1
plus the number of edges
in p
2
. We claim that if p is an optimal (i.e., shortest) path from u to ,then p
1
must be a shortest path from u to w. Why? We use a "cut-and-paste" argument:
if there were another path, say p
0
1
, from u to w with fewer edges than p
1
,thenwe
could cut out p
1
and paste in p
0
1
to produce a path u
p
0
1
 w
p
2
  with fewer edges
than p, thus contradicting p’s optimality. Symmetrically, p
2
must be a shortest
path from w to . Thus, we can find a shortest path from u to  by considering
all intermediate vertices w, finding a shortest path from u to w and a shortest path
from w to , and choosing an intermediate vertex w that yields the overall shortest
path. In Section 25.2, we use a variant of this observation of optimal substructure
to find a shortest path between every pair of vertices on a weighted, directed graph.
You might be tempted to assume that the problem of finding an unweighted
longest simple path exhibits optimal substructure as well. After all, if we decom-
pose a longest simple path u
p
  into subpaths u
p
1
 w
p
2
 , then mustn’t p
1
be a longest simple path from u to w, and mustn’t p
2
be a longest simple path
from w to ? The answer is no! Figure 15.6 supplies an example. Consider the
path q! r! t, which is a longest simple path from q to t.Is q! r a longest
simple path from q to r? No, for the path q! s ! t ! r isasimple path
that is longer. Is r! t a longest simple path from r to t? No again, for the path
r! q! s! t is a simple path that is longer.
15.3 Elements of dynamic programming 383
This example shows that for longest simple paths, not only does the problem
lack optimal substructure, but we cannot necessarily assemble a "legal" solution
to the problem from solutions to subproblems. If we combine the longest simple
paths q! s! t! r and r! q! s! t, we get the path q! s! t! r!
q! s! t, which is not simple. Indeed, the problem of finding an unweighted
longest simple path does not appear to have any sort of optimal substructure. No
efficient dynamic-programming algorithm for this problem has ever been found. In
fact, this problem is NP-complete, which-as we shall see in Chapter 34-means
that we are unlikely to find a way to solve it in polynomial time.
Why is the substructure of a longest simple path so different from that of a short-
est path? Although a solution to a problem for both longest and shortest paths uses
two subproblems, the subproblems in finding the longest simple path are notinde-
pendent, whereas for shortest paths they are. What do we mean by subproblems
being independent? We mean that the solution to one subproblem does not affect
the solution to another subproblem of the same problem. For the example of Fig-
ure 15.6, we have the problem of finding a longest simple path from q to t with two
subproblems: finding longest simple paths from q to r and from r to t. For the first
of these subproblems, we choose the path q! s! t! r, and so we have also
used the vertices s and t. We can no longer use these vertices in the second sub-
problem, since the combination of the two solutions to subproblems would yield a
path that is not simple. If we cannot use vertex t in the second problem, then we
cannot solve it at all, since t is required to be on the path that we find, and it is
not the vertex at which we are "splicing" together the subproblem solutions (that
vertex being r). Because we use vertices s and t in one subproblem solution, we
cannot use them in the other subproblem solution. We must use at least one of them
to solve the other subproblem, however, and we must use both of them to solve it
optimally. Thus, we say that these subproblems are not independent. Looked at
another way, using resources in solving one subproblem (those resources being
vertices) renders them unavailable for the other subproblem.
Why, then, are the subproblems independent for finding a shortest path? The
answer is that by nature, the subproblems do not share resources. We claim that
if a vertex w is on a shortest path p from u to , then we can splice together any
shortest path u
p
1
 w and any shortest path w
p
2
  to produce a shortest path from u
to . We are assured that, other than w, no vertex can appear in both paths p
1
and p
2
. Why? Suppose that some vertex x¤ w appears in both p
1
and p
2
,so that
we can decompose p
1
as u
p
ux
 x w and p
2
as w x
p
x
 . By the optimal
substructure of this problem, path p has as many edges as p
1
and p
2
together; let’s
say that p has e edges. Now let us construct a path p
0
D u
p
ux
 x
p
x
  from u to .
Because we have excised the paths from x to w and from w to x, each of which
contains at least one edge, path p
0
contains at most e 2 edges, which contradicts
384 Chapter 15 Dynamic Programming
the assumption that p is a shortest path. Thus, we are assured that the subproblems
for the shortest-path problem are independent.
Both problems examined in Sections 15.1 and 15.2 have independent subprob-
lems. In matrix-chain multiplication, the subproblems are multiplying subchains
A
i
A
iC1
 A
k
and A
kC1
A
kC2
 A
j
. These subchains are disjoint, so that no ma-
trix could possibly be included in both of them. In rod cutting, to determine the
best way to cut up a rod of length n, we look at the best ways of cutting up rods
of length i for iD 0; 1; : : : ; n 1. Because an optimal solution to the length-n
problem includes just one of these subproblem solutions (after we have cut off the
first piece), independence of subproblems is not an issue.
Overlappingsubproblems
The second ingredient that an optimization problem must have for dynamic pro-
gramming to apply is that the space of subproblems must be "small" in the sense
that a recursive algorithm for the problem solves the same subproblems over and
over, rather than always generating new subproblems. Typically, the total number
of distinct subproblems is a polynomial in the input size. When a recursive algo-
rithm revisits the same problem repeatedly, we say that the optimization problem
has overlapping subproblems.
4
In contrast, a problem for which a divide-and-
conquer approach is suitable usually generates brand-new problems at each step
of the recursion. Dynamic-programming algorithms typically take advantage of
overlapping subproblems by solving each subproblem once and then storing the
solution in a table where it can be looked up when needed, using constant time per
lookup.
In Section 15.1, we brieﬂy examined how a recursive solution to rod cut-
ting makes exponentially many calls to find solutions of smaller subproblems.
Our dynamic-programming solution takes an exponential-time recursive algorithm
down to quadratic time.
To illustrate the overlapping-subproblems property in greater detail, let us re-
examine the matrix-chain multiplication problem. Referring back to Figure 15.5,
observe that MATRIX-CHAIN-ORDER repeatedly looks up the solution to subprob-
lems in lower rows when solving subproblems in higher rows. For example, it
references entry mŒ3; 4 four times: during the computations of mŒ2; 4, mŒ1; 4,
4
It may seem strange that dynamic programming relies on subproblems being both independent
and overlapping. Although these requirements may sound contradictory, they describe two different
notions, rather than two points on the same axis. Two subproblems of the same problem are inde-
pendent if they do not share resources. Two subproblems are overlapping if they are really the same
subproblem that occurs as a subproblem of different problems.
15.3 Elements of dynamic programming 385
1..4
1..1 2..4 1..2 3..4 1..3 4..4
2..2 3..4 2..3 4..4 1..1 2..2 3..3 4..4 1..1 2..3 1..2 3..3
3..3 4..4 2..2 3..3 2..2 3..3 1..1 2..2
Figure 15.7 The recursion tree for the computation of RECURSIVE-MATRIX-CHAIN.p;1;4/.
Each node contains the parameters i and j . The computations performed in a shaded subtree are
replaced by a single table lookup in MEMOIZED-MATRIX-CHAIN.
mŒ3; 5,and mŒ3; 6. If we were to recompute mŒ3; 4 each time, rather than just
looking it up, the running time would increase dramatically. To see how, consider
the following (inefficient) recursive procedure that determines mŒi; j , the mini-
mum number of scalar multiplications needed to compute the matrix-chain product
A
i::j
D A
i
A
iC1
 A
j
. The procedure is based directly on the recurrence (15.7).
RECURSIVE-MATRIX-CHAIN.p;i;j/
1 if i == j
2 return 0
3 mŒi; j D1
4 for kD i to j 1
5 qD RECURSIVE-MATRIX-CHAIN.p;i;k/
C RECURSIVE-MATRIX-CHAIN.p; kC 1; j /
C p
i1
p
k
p
j
6 ifq<mŒi;j
7 mŒi; j D q
8 return mŒi; j 
Figure 15.7 shows the recursion tree produced by the call RECURSIVE-MATRIX-
CHAIN.p;1;4/. Each node is labeled by the values of the parameters i and j .
Observe that some pairs of values occur many times.
In fact, we can show that the time to compute mŒ1; n by this recursive proce-
dure is at least exponential in n.Let T .n/ denote the time taken by RECURSIVE-
MATRIX-CHAIN to compute an optimal parenthesization of a chain of n matrices.
Because the execution of lines 1–2 and of lines 6–7 each take at least unit time, as
386 Chapter 15 Dynamic Programming
does the multiplication in line 5, inspection of the procedure yields the recurrence
T.1/  1;
T .n/  1C
n1
X
kD1
.T .k/C T.n k/C 1/ forn>1:
Noting that for iD 1;2;:::;n 1, each term T.i/ appears once as T.k/ and once
as T.n k/, and collecting the n11s in the summation together with the 1 out
front, we can rewrite the recurrence as
T .n/ 2
n1
X
iD1
T.i/Cn: (15.8)
We shall prove that T .n/ D .2
n
/ using the substitution method. Specifi-
cally, we shall show that T .n/ 2
n1
for all n 1. The basis is easy, since
T.1/ 1D 2
0
. Inductively, for n 2 we have
T .n/  2
n1
X
iD1
2
i1
C n
D 2
n2
X
iD0
2
i
C n
D 2.2
n1
 1/C n (by equation (A.5))
D 2
n
 2C n
 2
n1
;
which completes the proof. Thus, the total amount of work performed by the call
RECURSIVE-MATRIX-CHAIN.p;1;n/ is at least exponential in n.
Compare this top-down, recursive algorithm (without memoization) with the
bottom-up dynamic-programming algorithm. The latter is more efficient because
it takes advantage of the overlapping-subproblems property. Matrix-chain mul-
tiplication has only ‚.n
2
/ distinct subproblems, and the dynamic-programming
algorithm solves each exactly once. The recursive algorithm, on the other hand,
must again solve each subproblem every time it reappears in the recursion tree.
Whenever a recursion tree for the natural recursive solution to a problem contains
the same subproblem repeatedly, and the total number of distinct subproblems is
small, dynamic programming can improve efficiency, sometimes dramatically.
15.3 Elements of dynamic programming 387
Reconstructinganoptimalsolution
As a practical matter, we often store which choice we made in each subproblem in
a table so that we do not have to reconstruct this information from the costs that we
stored.
For matrix-chain multiplication, the table sŒi;j saves us a significant amount of
work when reconstructing an optimal solution. Suppose that we did not maintain
the sŒi;j table, having filled in only the table mŒi; j  containing optimal subprob-
lem costs. We choose from among j i possibilities when we determine which
subproblems to use in an optimal solution to parenthesizing A
i
A
iC1
 A
j
,and
j i is not a constant. Therefore, it would take ‚.j i/D !.1/ time to recon-
struct which subproblems we chose for a solution to a given problem. By storing
in sŒi;j the index of the matrix at which we split the product A
i
A
iC1
 A
j
,we
can reconstruct each choice in O.1/ time.
Memoization
As we saw for the rod-cutting problem, there is an alternative approach to dy-
namic programming that often offers the efficiency of the bottom-up dynamic-
programming approach while maintaining a top-down strategy. The idea is to
memoize the natural, but inefficient, recursive algorithm. As in the bottom-up ap-
proach, we maintain a table with subproblem solutions, but the control structure
for filling in the table is more like the recursive algorithm.
A memoized recursive algorithm maintains an entry in a table for the solution to
each subproblem. Each table entry initially contains a special value to indicate that
the entry has yet to be filled in. When the subproblem is first encountered as the
recursive algorithm unfolds, its solution is computed and then stored in the table.
Each subsequent time that we encounter this subproblem, we simply look up the
value stored in the table and return it.
5
Here is a memoized version of RECURSIVE-MATRIX-CHAIN. Note where it
resembles the memoized top-down method for the rod-cutting problem.
5
This approach presupposes that we know the set of all possible subproblem parameters and that we
have established the relationship between table positions and subproblems. Another, more general,
approach is to memoize by using hashing with the subproblem parameters as keys.
388 Chapter 15 Dynamic Programming
MEMOIZED-MATRIX-CHAIN.p/
1 nD p:length 1
2let mŒ1::n;1::n be a new table
3 for iD 1to n
4 for j D i to n
5 mŒi; j D1
6 return LOOKUP-CHAIN.m;p;1;n/
LOOKUP-CHAIN.m;p;i;j/
1 if mŒi; j  <1
2 return mŒi; j 
3 if i == j
4 mŒi; j D 0
5 elsefor kD i to j 1
6 qD LOOKUP-CHAIN.m;p;i;k/
C LOOKUP-CHAIN.m;p;kC 1; j /C p
i1
p
k
p
j
7 ifq<mŒi;j
8 mŒi; j D q
9 return mŒi; j 
The MEMOIZED-MATRIX-CHAIN procedure, like MATRIX-CHAIN-ORDER,
maintains a tablemŒ1::n;1::n of computed values of mŒi; j , the minimum num-
ber of scalar multiplications needed to compute the matrix A
i::j
. Each table entry
initially contains the value1 to indicate that the entry has yet to be filled in. Upon
calling LOOKUP-CHAIN.m;p;i;j/, if line 1 finds that mŒi; j  <1, then the pro-
cedure simply returns the previously computed cost mŒi; j  in line 2. Otherwise,
the cost is computed as in RECURSIVE-MATRIX-CHAIN, stored in mŒi; j ,and
returned. Thus, LOOKUP-CHAIN.m;p;i;j/ always returns the value of mŒi; j ,
but it computes it only upon the first call of LOOKUP-CHAIN with these specific
values of i and j .
Figure 15.7 illustrates how MEMOIZED-MATRIX-CHAIN saves time compared
with RECURSIVE-MATRIX-CHAIN. Shaded subtrees represent values that it looks
up rather than recomputes.
Like the bottom-up dynamic-programming algorithm MATRIX-CHAIN-ORDER,
the procedure MEMOIZED-MATRIX-CHAIN runs in O.n
3
/ time. Line 5 of
MEMOIZED-MATRIX-CHAIN executes ‚.n
2
/ times. We can categorize the calls
of LOOKUP-CHAIN into two types:
1. calls in which mŒi; j D1, so that lines 3–9 execute, and
2. calls in which mŒi; j  <1,sothat LOOKUP-CHAIN simply returns in line 2.
15.3 Elements of dynamic programming 389
There are ‚.n
2
/ calls of the first type, one per table entry. All calls of the sec-
ond type are made as recursive calls by calls of the first type. Whenever a given
call of LOOKUP-CHAIN makes recursive calls, it makes O.n/ of them. There-
fore, there are O.n
3
/ calls of the second type in all. Each call of the second type
takes O.1/ time, and each call of the first type takes O.n/ time plus the time spent
in its recursive calls. The total time, therefore, is O.n
3
/. Memoization thus turns
an .2
n
/-time algorithm into an O.n
3
/-time algorithm.
In summary, we can solve the matrix-chain multiplication problem by either a
top-down, memoized dynamic-programming algorithm or a bottom-up dynamic-
programming algorithm in O.n
3
/ time. Both methods take advantage of the
overlapping-subproblems property. There are only ‚.n
2
/ distinct subproblems in
total, and either of these methods computes the solution to each subproblem only
once. Without memoization, the natural recursive algorithm runs in exponential
time, since solved subproblems are repeatedly solved.
In general practice, if all subproblems must be solved at least once, a bottom-up
dynamic-programming algorithm usually outperforms the corresponding top-down
memoized algorithm by a constant factor, because the bottom-up algorithm has no
overhead for recursion and less overhead for maintaining the table. Moreover, for
some problems we can exploit the regular pattern of table accesses in the dynamic-
programming algorithm to reduce time or space requirements even further. Alter-
natively, if some subproblems in the subproblem space need not be solved at all,
the memoized solution has the advantage of solving only those subproblems that
are definitely required.
Exercises
15.3-1
Which is a more efficient way to determine the optimal number of multiplications
in a matrix-chain multiplication problem: enumerating all the ways of parenthesiz-
ing the product and computing the number of multiplications for each, or running
RECURSIVE-MATRIX-CHAIN? Justify your answer.
15.3-2
Draw the recursion tree for the MERGE-SORT procedure from Section 2.3.1 on an
array of 16 elements. Explain why memoization fails to speed up a good divide-
and-conquer algorithm such as MERGE-SORT.
15.3-3
Consider a variant of the matrix-chain multiplication problem in which the goal is
to parenthesize the sequence of matrices so as to maximize, rather than minimize,
390 Chapter 15 Dynamic Programming
the number of scalar multiplications. Does this problem exhibit optimal substruc-
ture?
15.3-4
As stated, in dynamic programming we first solve the subproblems and then choose
which of them to use in an optimal solution to the problem. Professor Capulet
claims that we do not always need to solve all the subproblems in order to find an
optimal solution. She suggests that we can find an optimal solution to the matrix-
chain multiplication problem by always choosing the matrix A
k
at which to split
the subproduct A
i
A
iC1
 A
j
(by selecting k to minimize the quantity p
i1
p
k
p
j
)
before solving the subproblems. Find an instance of the matrix-chain multiplica-
tion problem for which this greedy approach yields a suboptimal solution.
15.3-5
Suppose that in the rod-cutting problem of Section 15.1, we also had limit l
i
on the
number of pieces of length i that we are allowed to produce, for iD 1;2;:::;n.
Show that the optimal-substructure property described in Section 15.1 no longer
holds.
15.3-6
Imagine that you wish to exchange one currency for another. You realize that
instead of directly exchanging one currency for another, you might be better off
making a series of trades through other currencies, winding up with the currency
you want. Suppose that you can trade n different currencies, numbered 1;2;:::;n,
where you start with currency 1 and wish to wind up with currency n.Youare
given, for each pair of currencies i and j , an exchange rate r
ij
, meaning that if
you start with d units of currency i, you can trade for dr
ij
units of currency j .
A sequence of trades may entail a commission, which depends on the number of
trades you make. Let c
k
be the commission that you are charged when you make k
trades. Show that, if c
k
D 0 for all kD 1;2;:::;n, then the problem of finding the
best sequence of exchanges from currency 1 to currency n exhibits optimal sub-
structure. Then show that if commissions c
k
are arbitrary values, then the problem
of finding the best sequence of exchanges from currency 1 to currency n does not
necessarily exhibit optimal substructure.
15.4 Longestcommonsubsequence
Biological applications often need to compare the DNA of two (or more) dif-
ferent organisms. A strand of DNA consists of a string of molecules called
15.4 Longest common subsequence 391
bases, where the possible bases are adenine, guanine, cytosine, and thymine.
Representing each of these bases by its initial letter, we can express a strand
of DNA as a string over the finite set fA;C;G;Tg. (See Appendix C for
the definition of a string.) For example, the DNA of one organism may be
S
1
DACCGGTCGAGTGCGCGGAAGCCGGCCGAA, and the DNA of another organ-
ism may be S
2
DGTCGTTCGGAATGCCGTTGCTCTGTAAA. One reason to com-
pare two strands of DNA is to determine how "similar" the two strands are, as some
measure of how closely related the two organisms are. We can, and do, define sim-
ilarity in many different ways. For example, we can say that two DNA strands are
similar if one is a substring of the other. (Chapter 32 explores algorithms to solve
this problem.) In our example, neither S
1
nor S
2
is a substring of the other. Alter-
natively, we could say that two strands are similar if the number of changes needed
to turn one into the other is small. (Problem 15-5 looks at this notion.) Yet another
way to measure the similarity of strands S
1
and S
2
is by finding a third strand S
3
in which the bases in S
3
appear in each of S
1
and S
2
; these bases must appear
in the same order, but not necessarily consecutively. The longer the strand S
3
we
can find, the more similar S
1
and S
2
are. In our example, the longest strand S
3
is
GTCGTCGGAAGCCGGCCGAA.
We formalize this last notion of similarity as the longest-common-subsequence
problem. A subsequence of a given sequence is just the given sequence with zero or
more elements left out. Formally, given a sequence XDhx
1
;x
2
;:::;x
m
i, another
sequence ZDh´
1
;´
2
; :::; ´
k
i is a subsequence of X if there exists a strictly
increasing sequencehi
1
;i
2
;:::;i
k
i of indices of X such that for all jD 1;2;:::;k,
we have x
i
j
D ´
j
. For example, ZDhB; C; D; Bi is a subsequence of X D
hA;B;C;B;D;A;Bi with corresponding index sequenceh2; 3; 5; 7i.
Given two sequences X and Y , we say that a sequence Z is a common sub-
sequence of X and Y if Z is a subsequence of both X and Y . For example, if
XDhA;B;C;B;D;A;Bi and YDhB;D;C;A;B;Ai, the sequencehB;C;Ai is
a common subsequence of both X and Y . The sequencehB;C; Ai is not a longest
common subsequence (LCS) of X and Y , however, since it has length 3 and the
sequencehB; C; B; Ai, which is also common to both X and Y , has length 4.The
sequencehB; C; B; Ai is an LCS of X and Y , as is the sequencehB; D; A; Bi,
since X and Y have no common subsequence of length 5 or greater.
In the longest-common-subsequence problem, we are given two sequences
XDhx
1
;x
2
; :::; x
m
i and YDhy
1
;y
2
; :::; y
n
i andwish tofind amaximum-
length common subsequence of X and Y . This section shows how to efficiently
solve the LCS problem using dynamic programming.
392 Chapter 15 Dynamic Programming
Step1: Characterizingalongestcommonsubsequence
In a brute-force approach to solving the LCS problem, we would enumerate all
subsequences of X and check each subsequence to see whether it is also a subse-
quence of Y , keeping track of the longest subsequence we find. Each subsequence
of X corresponds to a subset of the indicesf1;2;:::;mg of X. Because X has 2
m
subsequences, this approach requires exponential time, making it impractical for
long sequences.
The LCS problem has an optimal-substructure property, however, as the follow-
ing theorem shows. As we shall see, the natural classes of subproblems corre-
spond to pairs of "prefixes" of the two input sequences. To be precise, given a
sequence XDhx
1
;x
2
;:::;x
m
i,wedefinethe ithprefix of X,for iD 0; 1; : : : ; m,
as X
i
Dhx
1
;x
2
; :::; x
i
i. For example, if XDhA; B; C; B; D; A; Bi,then
X
4
DhA; B; C; Bi and X
0
is the empty sequence.
Theorem15.1(OptimalsubstructureofanLCS)
Let XDhx
1
;x
2
; :::;x
m
i and YDhy
1
;y
2
;:::; y
n
i be sequences, and let ZD
h´
1
;´
2
;:::;´
k
i be any LCS of X and Y .
1. If x
m
D y
n
,then ´
k
D x
m
D y
n
and Z
k1
is an LCS of X
m1
and Y
n1
.
2. If x
m
¤ y
n
,then ´
k
¤ x
m
implies that Z is an LCS of X
m1
and Y .
3. If x
m
¤ y
n
,then ´
k
¤ y
n
implies that Z is an LCS of X and Y
n1
.
Proof (1) If ´
k
¤ x
m
, then we could append x
m
D y
n
to Z to obtain a common
subsequence of X and Y of length kC 1, contradicting the supposition that Z is
a longest common subsequence of X and Y . Thus, we must have ´
k
D x
m
D y
n
.
Now, the prefix Z
k1
is a length-.k 1/ common subsequence of X
m1
and Y
n1
.
We wish to show that it is an LCS. Suppose for the purpose of contradiction
that there exists a common subsequence W of X
m1
and Y
n1
with length greater
than k 1. Then, appending x
m
D y
n
to W produces a common subsequence of
X and Y whose length is greater than k, which is a contradiction.
(2) If ´
k
¤ x
m
,then Z is a common subsequence of X
m1
and Y . If there were a
common subsequence W of X
m1
and Y with length greater than k,then W would
also be a common subsequence of X
m
and Y , contradicting the assumption that Z
is an LCS of X and Y .
(3) The proof is symmetric to (2).
The way that Theorem 15.1 characterizes longest common subsequences tells
us that an LCS of two sequences contains within it an LCS of prefixes of the two
sequences. Thus, the LCS problem has an optimal-substructure property. A recur-
15.4 Longest common subsequence 393
sive solution also has the overlapping-subproblems property, as we shall see in a
moment.
Step2: Arecursivesolution
Theorem 15.1 implies that we should examine either one or two subproblems when
finding an LCS of XDhx
1
;x
2
;:::;x
m
i and YDhy
1
;y
2
;:::;y
n
i.If x
m
D y
n
,
we must find an LCS of X
m1
and Y
n1
. Appending x
m
D y
n
to this LCS yields
an LCS of X and Y.If x
m
¤ y
n
, then we must solve two subproblems: finding an
LCS of X
m1
and Y and finding an LCS of X and Y
n1
. Whichever of these two
LCSs is longer is an LCS of X and Y . Because these cases exhaust all possibilities,
we know that one of the optimal subproblem solutions must appear within an LCS
of X and Y .
We can readily see the overlapping-subproblems property in the LCS problem.
To find an LCS of X and Y , we may need to find the LCSs of X and Y
n1
and
of X
m1
and Y . But each of these subproblems has the subsubproblem of finding
an LCS of X
m1
and Y
n1
. Many other subproblems share subsubproblems.
As in the matrix-chain multiplication problem, our recursive solution to the LCS
problem involves establishing a recurrence for the value of an optimal solution.
Let us define cŒi;j to be the length of an LCS of the sequences X
i
and Y
j
.If
either i D 0 or j D 0, one of the sequences has length 0, and so the LCS has
length 0. The optimal substructure of the LCS problem gives the recursive formula
cŒi;jD

0 if iD 0 or jD 0;
cŒi 1; j 1C 1 if i;j > 0 and x
i
D y
j
;
max.cŒi; j 1; cŒi 1; j / if i;j > 0 and x
i
¤ y
j
:
(15.9)
Observe that in this recursive formulation, a condition in the problem restricts
which subproblems we may consider. When x
i
D y
j
, we can and should consider
the subproblem of finding an LCS of X
i1
and Y
j 1
. Otherwise, we instead con-
sider the two subproblems of finding an LCS of X
i
and Y
j 1
and of X
i1
and Y
j
.In
the previous dynamic-programming algorithms we have examined-for rod cutting
and matrix-chain multiplication-we ruled out no subproblems due to conditions
in the problem. Finding an LCS is not the only dynamic-programming algorithm
that rules out subproblems based on conditions in the problem. For example, the
edit-distance problem (see Problem 15-5) has this characteristic.
Step3: ComputingthelengthofanLCS
Based on equation (15.9), we could easily write an exponential-time recursive al-
gorithm to compute the length of an LCS of two sequences. Since the LCS problem
394 Chapter 15 Dynamic Programming
has only ‚.mn/ distinct subproblems, however, we can use dynamic programming
to compute the solutions bottom up.
Procedure LCS-LENGTH takes two sequences XDhx
1
;x
2
; :::; x
m
i and
YDhy
1
;y
2
;:::;y
n
i as inputs. It stores the cŒi;j values in a table cŒ0::m;0::n,
and it computes the entries inrow-major order. (That is, the procedure fills in the
first row of c from left to right, then the second row, and so on.) The procedure also
maintains the table bŒ1::m;1::n to help us construct an optimal solution. Intu-
itively, bŒi;j points to the table entry corresponding to the optimal subproblem
solution chosen when computing cŒi;j. The procedure returns the b and c tables;
cŒm;n contains the length of an LCS of X and Y .
LCS-LENGTH.X; Y /
1 mD X:length
2 nD Y:length
3let bŒ1::m;1::n and cŒ0::m;0::n be new tables
4 for iD 1to m
5 cŒi;0D 0
6 for jD 0to n
7 cŒ0;jD 0
8 for iD 1to m
9 for jD 1to n
10 if x
i
== y
j
11 cŒi;jD cŒi 1; j 1C 1
12 bŒi;jD "-"
13 elseif cŒi 1; j  cŒi;j 1
14 cŒi;jD cŒi 1; j 
15 bŒi;jD """
16 else cŒi;jD cŒi;j 1
17 bŒi;jD " "
18 return c and b
Figure 15.8 shows the tables produced by LCS-LENGTH on the sequences XD
hA; B; C; B; D; A; Bi and YDhB; D; C; A; B; Ai. The running time of the
procedure is ‚.mn/, since each table entry takes ‚.1/ time to compute.
Step4: ConstructinganLCS
The b table returned by LCS-LENGTH enables us to quickly construct an LCS of
XDhx
1
;x
2
;:::;x
m
i and YDhy
1
;y
2
;:::;y
n
i. We simply begin at bŒm;n and
trace through the table by following the arrows. Whenever we encounter a "-"in
entry bŒi;j, it implies that x
i
D y
j
is an element of the LCS that LCS-LENGTH
15.4 Longest common subsequence 395
0 0 0 0 0 0 0
0 0 0 0 1 1 1
0 1 1 1 2 2
0 1 1 2 2 2
0 1 1 2 2 3
0 1 2 2 2 3 3
0 1 2 2 3 3
0 1 2 2 3 4 4
1
2
3
4
BD C A B A
123456 0
A
B
C
B
D
A
B
1
2
3
4
5
6
7
0
j
i
x
i
y
j
Figure15.8 The c and b tables computed by LCS-LENGTH on the sequences XDhA; B; C; B;
D;A;Bi and YDhB;D;C;A;B;Ai. The square in row i and column j contains the value of cŒi; j
and the appropriate arrow for the value of bŒi; j.Theentry 4 in cŒ7; 6-the lower right-hand corner
of the table-is the length of an LCShB; C; B; Ai of X and Y.For i; j > 0,entry cŒi; j depends
only on whether x
i
D y
j
and the values in entries cŒi 1; j , cŒi; j 1,and cŒi 1; j 1,which
are computed before cŒi; j. To reconstruct the elements of an LCS, follow the bŒi; j arrows from
the lower right-hand corner; the sequence is shaded. Each "-" on the shaded sequence corresponds
to an entry (highlighted) for which x
i
D y
j
is amemberofanLCS.
found. With this method, we encounter the elements of this LCS in reverse order.
The following recursive procedure prints out an LCS of X and Y in the proper,
forward order. The initial call is PRINT-LCS.b;X;X:length;Y:length/.
PRINT-LCS.b;X;i;j/
1 if i == 0 or j == 0
2 return
3 if bŒi;j == "-"
4PRINT-LCS.b;X;i 1; j 1/
5 print x
i
6 elseif bŒi;j == """
7PRINT-LCS.b;X;i 1; j /
8 else PRINT-LCS.b;X;i;j 1/
For the b table in Figure 15.8, this procedure prints BCBA. The procedure takes
time O.mC n/, since it decrements at least one of i and j in each recursive call.
396 Chapter 15 Dynamic Programming
Improvingthecode
Once you have developed an algorithm, you will often find that you can improve
on the time or space it uses. Some changes can simplify the code and improve
constant factors but otherwise yield no asymptotic improvement in performance.
Others can yield substantial asymptotic savings in time and space.
In the LCS algorithm, for example, we can eliminate the b table altogether. Each
cŒi;j entry depends on only three other c table entries: cŒi 1; j 1, cŒi 1; j ,
and cŒi;j 1. Given the value of cŒi;j, we can determine in O.1/ time which of
these three values was used to compute cŒi;j, without inspecting table b. Thus, we
can reconstruct an LCS in O.mCn/ time using a procedure similar to PRINT-LCS.
(Exercise 15.4-2 asks you to give the pseudocode.) Although we save ‚.mn/ space
by this method, the auxiliary space requirement for computing an LCS does not
asymptotically decrease, since we need ‚.mn/ space for the c table anyway.
We can, however, reduce the asymptotic space requirements for LCS-LENGTH,
since it needs only two rows of table c at a time: the row being computed and the
previous row. (In fact, as Exercise 15.4-4 asks you to show, we can use only slightly
more than the space for one row of c to compute the length of an LCS.) This
improvement works if we need only the length of an LCS; if we need to reconstruct
the elements of an LCS, the smaller table does not keep enough information to
retrace our steps in O.mC n/ time.
Exercises
15.4-1
Determine an LCS ofh1; 0; 0; 1; 0; 1; 0; 1i andh0; 1; 0; 1; 1; 0; 1; 1; 0i.
15.4-2
Give pseudocode to reconstruct an LCS from the completed c table and the original
sequences XDhx
1
;x
2
; :::; x
m
i and YDhy
1
;y
2
; :::; y
n
i in O.mC n/ time,
without using the b table.
15.4-3
Give a memoized version of LCS-LENGTH that runs in O.mn/ time.
15.4-4
Show how to compute the length of an LCS using only 2min.m; n/ entries in the c
table plus O.1/ additional space. Then show how to do the same thing, but using
min.m; n/ entries plus O.1/ additional space.
15.5 Optimal binary search trees 397
15.4-5
Give an O.n
2
/-time algorithm to find the longest monotonically increasing subse-
quence of a sequence of n numbers.
15.4-6 ?
Give an O.n lg n/-time algorithm to find the longest monotonically increasing sub-
sequence of a sequence of n numbers. (Hint: Observe that the last element of a
candidate subsequence of length i is at least as large as the last element of a can-
didate subsequence of length i 1. Maintain candidate subsequences by linking
them through the input sequence.)
15.5 Optimalbinarysearchtrees
Suppose that we are designing a program to translate text from English to French.
For each occurrence of each English word in the text, we need to look up its French
equivalent. We could perform these lookup operations by building a binary search
tree with n English words as keys and their French equivalents as satellite data.
Because we will search the tree for each individual word in the text, we want the
total time spent searching to be as low as possible. We could ensure an O.lg n/
search time per occurrence by using a red-black tree or any other balanced binary
search tree. Words appear with different frequencies, however, and a frequently
used word such as the may appear far from the root while a rarely used word such
as machicolation appears near the root. Such an organization would slow down the
translation, since the number of nodes visited when searching for a key in a binary
search tree equals one plus the depth of the node containing the key. We want
words that occur frequently in the text to be placed nearer the root.
6
Moreover,
some words in the text might have no French translation,
7
and such words would
not appear in the binary search tree at all. How do we organize a binary search tree
so as to minimize the number of nodes visited in all searches, given that we know
how often each word occurs?
What we need is known as an optimal binary search tree. Formally, we are
given a sequence KDhk
1
;k
2
;:::;k
n
i of n distinct keys in sorted order (so that
k
1
<k
2
< <k
n
), and we wish to build a binary search tree from these keys.
For each key k
i
, we have a probability p
i
that a search will be for k
i
.Some
searches may be for values not in K, and so we also have nC 1 "dummy keys"
6
If the subject of the text is castle architecture, we might want machicolation to appear near the root.
7
Yes, machicolation has a French counterpart: mˆ achicoulis.
398 Chapter 15 Dynamic Programming
k
2
k
1
k
4
k
3
k
5
d
0
d
1
d
2
d
3
d
4
d
5
(a)
k
2
k
1
k
4
k
3
k
5
d
0
d
1
d
2
d
3
d
4
d
5
(b)
Figure15.9 Two binary search trees for a set of nD 5 keys with the following probabilities:
i 01 234 5
p
i
0.15 0.10 0.05 0.10 0.20
q
i
0.05 0.10 0.05 0.05 0.05 0.10
(a) A binary search tree with expected search cost 2.80. (b) A binary search tree with expected search
cost 2.75. This tree is optimal.
d
0
;d
1
;d
2
;:::;d
n
representing values not in K. In particular, d
0
represents all val-
ues less than k
1
, d
n
represents all values greater than k
n
,andfor iD 1;2;:::;n1,
the dummy key d
i
represents all values between k
i
and k
iC1
. For each dummy
key d
i
, we have a probability q
i
that a search will correspond to d
i
. Figure 15.9
shows two binary search trees for a set of nD 5 keys. Each key k
i
is an internal
node, and each dummy key d
i
is a leaf. Every search is either successful (finding
some key k
i
) or unsuccessful (finding some dummy key d
i
), andsowehave
n
X
iD1
p
i
C
n
X
iD0
q
i
D 1: (15.10)
Because we have probabilities of searches for each key and each dummy key,
we can determine the expected cost of a search in a given binary search tree T.Let
us assume that the actual cost of a search equals the number of nodes examined,
i.e., the depth of the node found by the search in T , plus 1. Then the expected cost
of a search in T is
E Œsearch cost in TD
n
X
iD1
.depth
T
.k
i
/C 1/ p
i
C
n
X
iD0
.depth
T
.d
i
/C 1/ q
i
D 1C
n
X
iD1
depth
T
.k
i
/ p
i
C
n
X
iD0
depth
T
.d
i
/ q
i
; (15.11)
15.5 Optimal binary search trees 399
where depth
T
denotes a node’s depth in the tree T . The last equality follows from
equation (15.10). In Figure 15.9(a), we can calculate the expected search cost node
by node:
node depth probability contribution
k
1
1 0.15 0.30
k
2
0 0.10 0.10
k
3
2 0.05 0.15
k
4
1 0.10 0.20
k
5
2 0.20 0.60
d
0
2 0.05 0.15
d
1
2 0.10 0.30
d
2
3 0.05 0.20
d
3
3 0.05 0.20
d
4
3 0.05 0.20
d
5
3 0.10 0.40
Total 2.80
For a given set of probabilities, we wish to construct a binary search tree whose
expected search cost is smallest. We call such a tree anoptimalbinarysearchtree.
Figure 15.9(b) shows an optimal binary search tree for the probabilities given in
the figure caption; its expected cost is 2.75. This example shows that an optimal
binary search tree is not necessarily a tree whose overall height is smallest. Nor
can we necessarily construct an optimal binary search tree by always putting the
key with the greatest probability at the root. Here, key k
5
has the greatest search
probability of any key, yet the root of the optimal binary search tree shown is k
2
.
(The lowest expected cost of any binary search tree with k
5
at the root is 2.85.)
As with matrix-chain multiplication, exhaustive checking of all possibilities fails
to yield an efficient algorithm. We can label the nodes of any n-node binary tree
with the keys k
1
;k
2
;:::;k
n
to construct a binary search tree, and then add in the
dummy keys as leaves. In Problem 12-4, we saw that the number of binary trees
with n nodes is .4
n
=n
3=2
/, and so we would have to examine an exponential
number of binary search trees in an exhaustive search. Not surprisingly, we shall
solve this problem with dynamic programming.
Step1: Thestructureofanoptimalbinarysearchtree
To characterize the optimal substructure of optimal binary search trees, we start
with an observation about subtrees. Consider any subtree of a binary search tree.
It must contain keys in a contiguous range k
i
;:::;k
j
,for some 1 i j  n.
In addition, a subtree that contains keys k
i
;:::;k
j
must also have as its leaves the
dummy keys d
i1
;:::;d
j
.
Now we can state the optimal substructure: if an optimal binary search tree T
has a subtree T
0
containing keys k
i
;:::;k
j
, then this subtree T
0
must be optimal as
400 Chapter 15 Dynamic Programming
well for the subproblem with keys k
i
;:::;k
j
and dummy keys d
i1
;:::;d
j
.The
usual cut-and-paste argument applies. If there were a subtree T
00
whose expected
cost is lower than that of T
0
, then we could cut T
0
out of T and paste in T
00
,
resulting in a binary search tree of lower expected cost than T , thus contradicting
the optimality of T .
We need to use the optimal substructure to show that we can construct an opti-
mal solution to the problem from optimal solutions to subproblems. Given keys
k
i
;:::;k
j
, one of these keys, say k
r
(i  r  j ), is the root of an optimal
subtree containing these keys. The left subtree of the root k
r
contains the keys
k
i
;:::;k
r1
(and dummy keys d
i1
;:::;d
r1
), and the right subtree contains the
keys k
rC1
;:::;k
j
(and dummy keys d
r
;:::;d
j
). As long as we examine all candi-
date roots k
r
,where i r j , and we determine all optimal binary search trees
containing k
i
;:::;k
r1
and those containing k
rC1
;:::;k
j
, we are guaranteed that
we will find an optimal binary search tree.
There is one detail worth noting about "empty" subtrees. Suppose that in a
subtree with keys k
i
;:::;k
j
, we select k
i
as the root. By the above argument, k
i
’s
left subtree contains the keys k
i
;:::;k
i1
. We interpret this sequence as containing
no keys. Bear in mind, however, that subtrees also contain dummy keys. We adopt
the convention that a subtree containing keys k
i
;:::;k
i1
has no actual keys but
does contain the single dummy key d
i1
. Symmetrically, if we select k
j
as the root,
then k
j
’s right subtree contains the keys k
j C1
;:::;k
j
; this right subtree contains
no actual keys, but it does contain the dummy key d
j
.
Step2: Arecursivesolution
We are ready to define the value of an optimal solution recursively. We pick our
subproblem domain as finding an optimal binary search tree containing the keys
k
i
;:::;k
j
,where i  1, j  n,and j  i 1.(When j D i 1,there
are no actual keys; we have just the dummy key d
i1
.) Let us define eŒi;j as
the expected cost of searching an optimal binary search tree containing the keys
k
i
;:::;k
j
. Ultimately, we wish to compute eŒ1;n.
The easy case occurs when jD i 1. Then we have just the dummy key d
i1
.
The expected search cost is eŒi;i 1D q
i1
.
When j i, we need to select a root k
r
from among k
i
;:::;k
j
andthenmakean
optimal binary search tree with keys k
i
;:::;k
r1
as its left subtree and an optimal
binary search tree with keys k
rC1
;:::;k
j
as its right subtree. What happens to the
expected search cost of a subtree when it becomes a subtree of a node? The depth
of each node in the subtree increases by 1. By equation (15.11), the expected search
cost of this subtree increases by the sum of all the probabilities in the subtree. For
a subtree with keys k
i
;:::;k
j
, let us denote this sum of probabilities as
15.5 Optimal binary search trees 401
w.i;j/D
j
X
lDi
p
l
C
j
X
lDi1
q
l
: (15.12)
Thus, if k
r
is the root of an optimal subtree containing keys k
i
;:::;k
j
,wehave
eŒi;jD p
r
C .eŒi; r 1C w.i;r 1//C .eŒrC 1; j C w.rC 1; j // :
Noting that
w.i;j/D w.i;r 1/C p
r
C w.rC 1; j / ;
we rewrite eŒi;j as
eŒi;jD eŒi;r 1C eŒrC 1; j C w.i;j/ : (15.13)
The recursive equation (15.13) assumes that we know which node k
r
to use as
the root. We choose the root that gives the lowest expected search cost, giving us
our final recursive formulation:
eŒi;jD
(
q
i1
if jD i1;
min
irj
feŒi;r 1C eŒrC 1; j C w.i;j/g if ij:
(15.14)
The eŒi;j values give the expected search costs in optimal binary search trees.
To help us keep track of the structure of optimal binary search trees, we define
rootŒi; j ,for 1 i  j  n, to be the index r for which k
r
is the root of an
optimal binary search tree containing keys k
i
;:::;k
j
. Although we will see how
to compute the values of rootŒi; j , we leave the construction of an optimal binary
search tree from these values as Exercise 15.5-1.
Step3: Computingtheexpectedsearchcostofanoptimalbinarysearchtree
At this point, you may have noticed some similarities between our characterizations
of optimal binary search trees and matrix-chain multiplication. For both problem
domains, our subproblems consist of contiguous index subranges. A direct, recur-
sive implementation of equation (15.14) would be as inefficient as a direct, recur-
sive matrix-chain multiplication algorithm. Instead, we store the eŒi;j values in a
table eŒ1: :nC1;0::n. The first index needs to run to nC1 rather than n because
in order to have a subtree containing only the dummy key d
n
, we need to compute
and store eŒnC 1; n. The second index needs to start from 0 because in order to
have a subtree containing only the dummy key d
0
, we need to compute and store
eŒ1; 0. We use only the entries eŒi;j for which j  i 1. We also use a table
rootŒi; j , for recording the root of the subtree containing keys k
i
;:::;k
j
.This
table uses only the entries for which 1 i j n.
We will need one other table for efficiency. Rather than compute the value
of w.i;j/ from scratch every time we are computing eŒi;j-which would take
402 Chapter 15 Dynamic Programming
‚.j i/ additions-we store these values in a table wŒ1: :nC 1;0::n.Forthe
base case, we compute wŒi;i 1D q
i1
for 1 i  nC 1.For j  i,we
compute
wŒi;jD wŒi;j 1C p
j
C q
j
: (15.15)
Thus, we can compute the ‚.n
2
/ values of wŒi;j in ‚.1/ time each.
The pseudocode that follows takes as inputs the probabilities p
1
;:::;p
n
and
q
0
;:::;q
n
and the size n, and it returns the tables e and root.
OPTIMAL-BST.p;q;n/
1let eŒ1: :nC1;0::n, wŒ1: :nC 1;0::n,
and rootŒ1::n;1::n be new tables
2 for iD 1to nC 1
3 eŒi;i 1D q
i1
4 wŒi;i 1D q
i1
5 for lD 1to n
6 for iD 1to n lC 1
7 j D iC l 1
8 eŒi;jD1
9 wŒi;jD wŒi;j 1C p
j
C q
j
10 for rD i to j
11 tD eŒi;r 1C eŒrC 1; j C wŒi;j
12 ift<eŒi;j
13 eŒi;jD t
14 rootŒi; j D r
15 return e and root
From the description above and the similarity to the MATRIX-CHAIN-ORDER pro-
cedure in Section 15.2, you should find the operation of this procedure to be fairly
straightforward. The for loop of lines 2–4 initializes the values of eŒi;i 1
and wŒi;i 1.The for loop of lines 5–14 then uses the recurrences (15.14)
and (15.15) to compute eŒi;j and wŒi;j for all 1 i j n. In the first itera-
tion, when lD 1, the loop computes eŒi;i and wŒi;i for iD 1;2;:::;n. The sec-
ond iteration, with lD 2, computes eŒi;iC1 and wŒi;iC1 for iD 1;2;:::;n1,
and so forth. The innermost for loop, in lines 10–14, tries each candidate index r
to determine which key k
r
to use as the root of an optimal binary search tree con-
taining keys k
i
;:::;k
j
.This for loop saves the current value of the index r in
rootŒi; j  whenever it finds a better key to use as the root.
Figure 15.10 shows the tables eŒi;j, wŒi;j,and rootŒi; j  computed by the
procedure OPTIMAL-BST on the key distribution shown in Figure 15.9. As in the
matrix-chain multiplication example of Figure 15.5, the tables are rotated to make
15.5 Optimal binary search trees 403
2.75
1.75
1.25
0.90
0.45
0.05
2.00
1.20
0.70
0.40
0.10
1.30
0.60
0.25
0.05
0.90
0.30
0.05
0.50
0.05 0.10
e
0
1
2
3
4
5
6
5
4
3
2
1
ji
1.00
0.70
0.55
0.45
0.30
0.05
0.80
0.50
0.35
0.25
0.10
0.60
0.30
0.15
0.05
0.50
0.20
0.05
0.35
0.05 0.10
w
0
1
2
3
4
5
6
5
4
3
2
1
ji
2
2
2
1
1
4
2
2
2
5
4
3
5
4 5
root
1
2
3
4
5
5
4
3
2
1
ji
Figure 15.10 The tables eŒi; j, wŒi; j,and rootŒi; j  computed by OPTIMAL-BST on the key
distribution shown in Figure 15.9. The tables are rotated so that the diagonals run horizontally.
the diagonals run horizontally. OPTIMAL-BST computes the rows from bottom to
top and from left to right within each row.
The OPTIMAL-BST procedure takes ‚.n
3
/ time, just like MATRIX-CHAIN-
ORDER. We can easily see that its running time is O.n
3
/, since its for loops are
nested three deep and each loop index takes on at most n values. The loop indices in
OPTIMAL-BST do not have exactly the same bounds as those in MATRIX-CHAIN-
ORDER, but they are within at most 1 in all directions. Thus, like MATRIX-CHAIN-
ORDER,the OPTIMAL-BST procedure takes .n
3
/ time.
Exercises
15.5-1
Write pseudocode for the procedure CONSTRUCT-OPTIMAL-BST.root/ which,
given the table root, outputs the structure of an optimal binary search tree. For the
example in Figure 15.10, your procedure should print out the structure
404 Chapter 15 Dynamic Programming
k
2
is the root
k
1
is the left child of k
2
d
0
is the left child of k
1
d
1
is the right child of k
1
k
5
is the right child of k
2
k
4
is the left child of k
5
k
3
is the left child of k
4
d
2
is the left child of k
3
d
3
is the right child of k
3
d
4
is the right child of k
4
d
5
is the right child of k
5
corresponding to the optimal binary search tree shown in Figure 15.9(b).
15.5-2
Determine the cost and structure of an optimal binary search tree for a set of nD 7
keys with the following probabilities:
i 01 234 56 7
p
i
0.04 0.06 0.08 0.02 0.10 0.12 0.14
q
i
0.06 0.06 0.06 0.06 0.05 0.05 0.05 0.05
15.5-3
Suppose that instead of maintaining the table wŒi;j, we computed the value
of w.i;j/ directly from equation (15.12) in line 9 of OPTIMAL-BST and used this
computed value in line 11. How would this change affect the asymptotic running
time of OPTIMAL-BST?
15.5-4 ?
Knuth [212] has shown that there are always roots of optimal subtrees such that
rootŒi; j 1 rootŒi; j  rootŒiC 1; j  for all 1i<j n. Use this fact to
modify the OPTIMAL-BST procedure to run in ‚.n
2
/ time.
Problems
15-1 Longestsimplepathinadirectedacyclicgraph
Suppose that we are given a directed acyclic graph G D .V; E/ with real-
valued edge weights and two distinguished vertices s and t. Describe a dynamic-
programming approach for finding a longest weighted simple path from s to t.
What does the subproblem graph look like? What is the efficiency of your algo-
rithm?
Problems for Chapter 15 405
(a) (b)
Figure 15.11 Seven points in the plane, shown on a unit grid. (a) The shortest closed tour, with
length approximately 24:89. This tour is not bitonic.(b)Theshortestbitonictourforthesamesetof
points. Its length is approximately 25:58.
15-2 Longestpalindromesubsequence
A palindrome is a nonempty string over some alphabet that reads the same for-
ward and backward. Examples of palindromes are all strings of length 1,civic,
racecar,andaibohphobia (fear of palindromes).
Give an efficient algorithm to find the longest palindrome that is a subsequence
of a given input string. For example, given the inputcharacter, your algorithm
should returncarac. What is the running time of your algorithm?
15-3 Bitoniceuclideantraveling-salesmanproblem
In the euclidean traveling-salesman problem,we are given a set of n points in
the plane, and we wish to find the shortest closed tour that connects all n points.
Figure 15.11(a) shows the solution to a 7-point problem. The general problem is
NP-hard, and its solution is therefore believed to require more than polynomial
time (see Chapter 34).
J. L. Bentley has suggested that we simplify the problem by restricting our at-
tention to bitonic tours, that is, tours that start at the leftmost point, go strictly
rightward to the rightmost point, and then go strictly leftward back to the starting
point. Figure 15.11(b) shows the shortest bitonic tour of the same 7 points. In this
case, a polynomial-time algorithm is possible.
Describe an O.n
2
/-time algorithm for determining an optimal bitonic tour. You
may assume that no two points have the same x-coordinate and that all operations
on real numbers take unit time. (Hint: Scan left to right, maintaining optimal pos-
sibilities for the two parts of the tour.)
15-4 Printingneatly
Consider the problem of neatly printing a paragraph with a monospaced font (all
characters having the same width) on a printer. The input text is a sequence of n
406 Chapter 15 Dynamic Programming
words of lengths l
1
;l
2
;:::;l
n
, measured in characters. We want to print this para-
graph neatly on a number of lines that hold a maximum of M characters each. Our
criterion of "neatness" is as follows. If a given line contains words i through j ,
where i j , and we leave exactly one space between words, the number of extra
space characters at the end of the line is M jC i
P
j
kDi
l
k
, which must be
nonnegative so that the words fit on the line. We wish to minimize the sum, over
all lines except the last, of the cubes of the numbers of extra space characters at the
ends of lines. Give a dynamic-programming algorithm to print a paragraph of n
words neatly on a printer. Analyze the running time and space requirements of
your algorithm.
15-5 Editdistance
In order to transform one source string of text xŒ1::m to a target string yŒ1::n,
we can perform various transformation operations. Our goal is, given x and y,
to produce a series of transformations that change x to y. We use an ar-
ray ´-assumed to be large enough to hold all the characters it will need-to hold
the intermediate results. Initially, ´ is empty, and at termination, we should have
´Œj D yŒj for jD 1;2;:::;n. We maintain current indices i into x and j into ´,
and the operations are allowed to alter ´ and these indices. Initially, iD jD 1.
We are required to examine every character in x during the transformation, which
means that at the end of the sequence of transformation operations, we must have
iD mC 1.
We may choose from among six transformation operations:
Copy a character from x to ´ by setting ´Œj D xŒi and then incrementing both i
and j . This operation examines xŒi.
Replace a character from x by another character c, by setting ´Œj D c,and then
incrementing both i and j . This operation examines xŒi.
Delete a character from x by incrementing i but leaving j alone. This operation
examines xŒi.
Insert the character c into ´ by setting ´Œj  D c and then incrementing j,but
leaving i alone. This operation examines no characters of x.
Twiddle (i.e., exchange) the next two characters by copying them from x to ´ but
in the opposite order; we do so by setting ´Œj D xŒiC 1 and ´ŒjC 1D xŒi
and then setting i D iC 2 and j D jC 2. This operation examines xŒi
and xŒiC 1.
Kill the remainder of x by setting i D mC 1. This operation examines all char-
acters in x that have not yet been examined. This operation, if performed, must
be the final operation.
Problems for Chapter 15 407
As an example, one way to transform the source stringalgorithm to the target
string altruistic is to use the following sequence of operations, where the
underlined characters are xŒi and ´Œj  after the operation:
Operationx´
initial strings algorithm
copy algorithm a
copy algorithm al
replace byt algorithm alt
delete algorithm alt
copy algorithm altr
insertu algorithm altru
inserti algorithm altrui
inserts algorithm altruis
twiddle algorithm altruisti
insertc algorithm altruistic
kill algorithm altruistic
Note that there are several other sequences of transformation operations that trans-
formalgorithm toaltruistic.
Each of the transformation operations has an associated cost. The cost of an
operation depends on the specific application, but we assume that each operation’s
cost is a constant that is known to us. We also assume that the individual costs of
the copy and replace operations are less than the combined costs of the delete and
insert operations; otherwise, the copy and replace operations would not be used.
The cost of a given sequence of transformation operations is the sum of the costs
of the individual operations in the sequence. For the sequence above, the cost of
transformingalgorithm toaltruistic is
.3 cost.copy//C cost.replace/C cost.delete/C .4 cost.insert//
C cost.twiddle/C cost.kill/:
a. Given two sequences xŒ1::m and yŒ1::n and set of transformation-operation
costs, theeditdistance from x to y is the cost of the least expensive operation
sequence that transforms x to y. Describe a dynamic-programming algorithm
that finds the edit distance from xŒ1::m to yŒ1::n and prints an optimal op-
eration sequence. Analyze the running time and space requirements of your
algorithm.
The edit-distance problem generalizes the problem of aligning two DNA sequences
(see, for example, Setubal and Meidanis [310, Section 3.2]). There are several
methods for measuring the similarity of two DNA sequences by aligning them.
One such method to align two sequences x and y consists of inserting spaces at
408 Chapter 15 Dynamic Programming
arbitrary locations in the two sequences (including at either end) so that the result-
ing sequences x
0
and y
0
have the same length but do not have a space in the same
position (i.e., for no position j are both x
0
Œj  and y
0
Œj  a space). Then we assign a
"score" to each position. Position j receives a score as follows:

C1 if x
0
Œj D y
0
Œj  and neither is a space,

1 if x
0
Œj ¤ y
0
Œj  and neither is a space,

2 if either x
0
Œj  or y
0
Œj  is a space.
The score for the alignment is the sum of the scores of the individual positions. For
example, given the sequences xD GATCGGCAT and yD CAATGTGAATC, one
alignment is
G ATCG GCAT
CAAT GTGAATC
-
*
++
*
+
*
+-++
*
A+ under a position indicates a score ofC1 for that position, a- indicates a score
of1,and a
*
indicates a score of2, so that this alignment has a total score of
6 1 2 1 4 2D4.
b. Explain how to cast the problem of finding an optimal alignment as an edit
distance problem using a subset of the transformation operations copy, replace,
delete, insert, twiddle, and kill.
15-6 Planningacompanyparty
Professor Stewart is consulting for the president of a corporation that is planning
a company party. The company has a hierarchical structure; that is, the supervisor
relation forms a tree rooted at the president. The personnel office has ranked each
employee with a conviviality rating, which is a real number. In order to make the
party fun for all attendees, the president does not want both an employee and his
or her immediate supervisor to attend.
Professor Stewart is given the tree that describes the structure of the corporation,
using the left-child, right-sibling representation described in Section 10.4. Each
node of the tree holds, in addition to the pointers, the name of an employee and
that employee’s conviviality ranking. Describe an algorithm to make up a guest
list that maximizes the sum of the conviviality ratings of the guests. Analyze the
running time of your algorithm.
15-7 Viterbialgorithm
We can use dynamic programming on a directed graph G D .V; E/ for speech
recognition. Each edge .u; / 2 E is labeled with a sound 	.u;/ from a fi-
nite set † of sounds. The labeled graph is a formal model of a person speaking
Problems for Chapter 15 409
a restricted language. Each path in the graph starting from a distinguished ver-
tex 
0
2 V corresponds to a possible sequence of sounds produced by the model.
We define the label of a directed path to be the concatenation of the labels of the
edges on that path.
a. Describe an efficient algorithm that, given an edge-labeled graph G with dis-
tinguished vertex 
0
and a sequence sDh	
1
;	
2
; :::; 	
k
i of sounds from †,
returns a path in G that begins at 
0
and has s as its label, if any such path exists.
Otherwise, the algorithm should return NO-SUCH-PATH. Analyze the running
time of your algorithm. (Hint: You may find concepts from Chapter 22 useful.)
Now, suppose that every edge .u; /2 E has an associated nonnegative proba-
bility p.u;/ of traversing the edge .u; / from vertex u and thus producing the
corresponding sound. The sum of the probabilities of the edges leaving any vertex
equals 1. The probability of a path is defined to be the product of the probabil-
ities of its edges. We can view the probability of a path beginning at 
0
as the
probability that a "random walk" beginning at 
0
will follow the specified path,
where we randomly choose which edge to take leaving a vertex u according to the
probabilities of the available edges leaving u.
b. Extend your answer to part (a) so that if a path is returned, it is a most prob-
able path starting at 
0
and having label s. Analyze the running time of your
algorithm.
15-8 Imagecompressionbyseamcarving
We are given a color picture consisting of an m	 n array AŒ1 : : m; 1 : : n of pixels,
where each pixel specifies a triple of red, green, and blue (RGB) intensities. Sup-
pose that we wish to compress this picture slightly. Specifically, we wish to remove
one pixel from each of the m rows, so that the whole picture becomes one pixel
narrower. To avoid disturbing visual effects, however, we require that the pixels
removed in two adjacent rows be in the same or adjacent columns; the pixels re-
moved form a "seam" from the top row to the bottom row where successive pixels
in the seam are adjacent vertically or diagonally.
a. Show that the number of such possible seams grows at least exponentially in m,
assuming thatn>1.
b. Suppose now that along with each pixel AŒi; j , we have calculated a real-
valued disruption measure dŒi;j, indicating how disruptive it would be to
remove pixel AŒi; j . Intuitively, the lower a pixel’s disruption measure, the
more similar the pixel is to its neighbors. Suppose further that we define the
disruption measure of a seam to be the sum of the disruption measures of its
pixels.
410 Chapter 15 Dynamic Programming
Give an algorithm to find a seam with the lowest disruption measure. How
efficient is your algorithm?
15-9 Breakingastring
A certain string-processing language allows a programmer to break a string into
two pieces. Because this operation copies the string, it costs n time units to break
a string of n characters into two pieces. Suppose a programmer wants to break
a string into many pieces. The order in which the breaks occur can affect the
total amount of time used. For example, suppose that the programmer wants to
break a 20-character string after characters 2, 8,and 10 (numbering the characters
in ascending order from the left-hand end, starting from 1). If she programs the
breaks to occur in left-to-right order, then the first break costs 20 time units, the
second break costs 18 time units (breaking the string from characters 3 to 20 at
character 8), and the third break costs 12 time units, totaling 50 time units. If she
programs the breaks to occur in right-to-left order, however, then the first break
costs 20 time units, the second break costs 10 time units, and the third break costs
8 time units, totaling 38 time units. In yet another order, she could break first at 8
(costing 20), then break the left piece at 2 (costing 8), and finally the right piece
at 10 (costing 12), for a total cost of 40.
Design an algorithm that, given the numbers of characters after which to break,
determines a least-cost way to sequence those breaks. More formally, given a
string S with n characters and an array LŒ1::m containing the break points, com-
pute the lowest cost for a sequence of breaks, along with a sequence of breaks that
achieves this cost.
15-10 Planninganinvestmentstrategy
Your knowledge of algorithms helps you obtain an exciting job with the Acme
Computer Company, along with a $10,000 signing bonus. You decide to invest
this money with the goal of maximizing your return at the end of 10 years. You
decide to use the Amalgamated Investment Company to manage your investments.
Amalgamated Investments requires you to observe the following rules. It offers n
different investments, numbered 1 through n. In each year j , investment i provides
a return rate of r
ij
. In other words, if you invest d dollars in investment i in year j ,
then at the end of year j , you have dr
ij
dollars. The return rates are guaranteed,
that is, you are given all the return rates for the next 10 years for each investment.
You make investment decisions only once per year. At the end of each year, you
can leave the money made in the previous year in the same investments, or you
can shift money to other investments, by either shifting money between existing
investments or moving money to a new investement. If you do not move your
money between two consecutive years, you pay a fee of f
1
dollars, whereas if you
switch your money, you pay a fee of f
2
dollars, where f
2
>f
1
.
Problems for Chapter 15 411
a. The problem, as stated, allows you to invest your money in multiple investments
in each year. Prove that there exists an optimal investment strategy that, in
each year, puts all the money into a single investment. (Recall that an optimal
investment strategy maximizes the amount of money after 10 years and is not
concerned with any other objectives, such as minimizing risk.)
b. Prove that the problem of planning your optimal investment strategy exhibits
optimal substructure.
c. Design an algorithm that plans your optimal investment strategy. What is the
running time of your algorithm?
d. Suppose that Amalgamated Investments imposed the additional restriction that,
at any point, you can have no more than $15,000 in any one investment. Show
that the problem of maximizing your income at the end of 10 years no longer
exhibits optimal substructure.
15-11 Inventoryplanning
The Rinky Dink Company makes machines that resurface ice rinks. The demand
for such products varies from month to month, and so the company needs to de-
velop a strategy to plan its manufacturing given the ﬂuctuating, but predictable,
demand. The company wishes to design a plan for the next n months. For each
month i, the company knows the demand d
i
, that is, the number of machines that
it will sell. Let DD
P
n
iD1
d
i
be the total demand over the next n months. The
company keeps a full-time staff who provide labor to manufacture up to m ma-
chines per month. If the company needs to make more than m machines in a given
month, it can hire additional, part-time labor, at a cost that works out to c dollars
per machine. Furthermore, if, at the end of a month, the company is holding any
unsold machines, it must pay inventory costs. The cost for holding j machines is
given as a function h.j / for jD 1;2;:::;D,where h.j / 0 for 1 j D and
h.j / h.jC 1/ for 1 j D 1.
Give an algorithm that calculates a plan for the company that minimizes its costs
while fulfilling all the demand. The running time should be polyomial in n and D.
15-12 Signingfree-agentbaseballplayers
Suppose that you are the general manager for a major-league baseball team. During
the off-season, you need to sign some free-agent players for your team. The team
owner has given you a budget of $X to spend on free agents. You are allowed to
spend less than $X altogether, but the owner will fire you if you spend any more
than $X.
412 Chapter 15 Dynamic Programming
You are considering N different positions, and for each position, P free-agent
players who play that position are available.
8
Because you do not want to overload
your roster with too many players at any position, for each position you may sign
at most one free agent who plays that position. (If you do not sign any players at a
particular position, then you plan to stick with the players you already have at that
position.)
To determine how valuable a player is going to be, you decide to use a sabermet-
ric statistic
9
known as "VORP," or "value over replacement player." A player with
a higher VORP is more valuable than a player with a lower VORP. A player with a
higher VORP is not necessarily more expensive to sign than a player with a lower
VORP, because factors other than a player’s value determine how much it costs to
sign him.
For each available free-agent player, you have three pieces of information:

the player’s position,

the amount of money it will cost to sign the player, and

the player’s VORP.
Devise an algorithm that maximizes the total VORP of the players you sign while
spending no more than $X altogether. You may assume that each player signs for a
multiple of $100,000. Your algorithm should output the total VORP of the players
you sign, the total amount of money you spend, and a list of which players you
sign. Analyze the running time and space requirement of your algorithm.
Chapternotes
R. Bellman began the systematic study of dynamic programming in 1955. The
word "programming," both here and in linear programming, refers to using a tab-
ular solution method. Although optimization techniques incorporating elements of
dynamic programming were known earlier, Bellman provided the area with a solid
mathematical basis [37].
8
Although there are nine positions on a baseball team, N is not necesarily equal to 9 because some
general managers have particular ways of thinking about positions. For example, a general manager
might consider right-handed pitchers and left-handed pitchers to be separate "positions," as well as
starting pitchers, long relief pitchers (relief pitchers who can pitch several innings), and short relief
pitchers (relief pitchers who normally pitch at most only one inning).
9
Sabermetrics is the application of statistical analysis to baseball records. It provides several ways
to compare the relative values of individual players.
Notes for Chapter 15 413
Galil and Park [125] classify dynamic-programming algorithms according to the
size of the table and the number of other table entries each entry depends on. They
call a dynamic-programming algorithm tD=eD if its table size is O.n
t
/ and each
entry depends on O.n
e
/ other entries. For example, the matrix-chain multiplication
algorithm in Section 15.2 would be 2D=1D, and the longest-common-subsequence
algorithm in Section 15.4 would be 2D=0D.
Hu and Shing [182, 183] give an O.n lg n/-time algorithm for the matrix-chain
multiplication problem.
The O.mn/-time algorithm for the longest-common-subsequence problem ap-
pears to be a folk algorithm. Knuth [70] posed the question of whether subquadratic
algorithms for the LCS problem exist. Masek and Paterson [244] answered this
question in the affirmative by giving an algorithm that runs in O.mn= lg n/ time,
where n m and the sequences are drawn from a set of bounded size. For the
special case in which no element appears more than once in an input sequence,
Szymanski [326] shows how to solve the problem in O..nC m/ lg.nC m// time.
Many of these results extend to the problem of computing string edit distances
(Problem 15-5).
An early paper on variable-length binary encodings by Gilbert and Moore [133]
had applications to constructing optimal binary search trees for the case in which all
probabilities p
i
are 0; this paper contains an O.n
3
/-time algorithm. Aho, Hopcroft,
and Ullman [5] present the algorithm from Section 15.5. Exercise 15.5-4 is due to
Knuth [212]. Hu and Tucker [184] devised an algorithm for the case in which all
probabilities p
i
are 0 that uses O.n
2
/ time and O.n/ space; subsequently, Knuth
[211] reduced the time to O.n lg n/.
Problem 15-8 is due to Avidan and Shamir [27], who have posted on the Web a
wonderful video illustrating this image-compression technique.
